# AI Futures: The Age of Exploration

_Rough sketches of our possible AI futures_

- _This is a story of a possible future where AI co-exist with humans_
    - _This is not a prediction, do not over-analyse_
- _There are many possible futures as various Sci-Fi books have shown, some good and some bad_
- _If you are reading this a few years in the future, disregard it. My thoughts on the matter may have changed._

---

### Summary:

- The world will increasing shift focus towards Creativity (possibility space expansion) from the Productivity (survival) due to the growing abundance in Intellect

- Reasons to be hopeful:
    - Many harmful human tendencies will be reduced if widespread abundance is achieved

---

The central tension in the development of AI is the balance between **Exploration** (Creativity) and **Exploitation** (Productivity).

- The Exploration-Exploitation Trade-off:

    All organism (artificial or otherwise) at each time-step must decide to either update their understanding of the world (**Explore**) or survive (**Exploit**). Successful AI systems learn the optimal strategy of when to make either choices.

The increasing abundance of both energy and intellect in the coming decade would shift us towards greater freedoms (**Exploration**) from a focus on survival (**Exploitation**).

- ### Energy: Solar (and other renewables) compared to our limited reserve of fossil fuels

- ### Intellect: Artificial Intelligence systems are performing human-like tasks:
    - Medical Research ([AlphaFold](https://www.deepmind.com/research/highlighted-research/alphafold))
    - Game of Go ([AlphaGo](https://www.deepmind.com/research/highlighted-research/alphago))
    - Game of Diplomacy, Strategy, Human Cooperation ([Cicero](https://github.com/facebookresearch/diplomacy_cicero))
    - Dialogue Simulation ([ChatGPT](https://openai.com/blog/chatgpt), [LaMDA](https://blog.google/technology/ai/lamda), [LLaMA](https://github.com/facebookresearch/llama))
    - Image and Video Generation from text descriptions ([DALL-E 2](https://openai.com/dall-e-2), [Midjourney](https://www.midjourney.com), [Stable Diffusion](https://github.com/Stability-AI/StableDiffusion), [Imagen Video](https://imagen.research.google/video), [Muse](https://muse-model.github.io), [Runway Gen-2](https://research.runwayml.com/gen2))
    - Real time Decision and Planning, Robotics ([Atlas](https://www.youtube.com/watch?v=XPVC4IyRTG8))
    - Real time Learning ([Adaptive Agent](https://sites.google.com/view/adaptive-agent))
    - Real world Learning ([Toolformer](https://github.com/lucidrains/toolformer-pytorch), [Langchain](https://github.com/hwchase17/langchain), [Internet Explorer](https://internet-explorer-ssl.github.io))
    - Some Generalisation ability, Multi-model ([Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model), [Cato](https://www.deepmind.com/publications/a-generalist-agent), [Multimodal-CoT](https://github.com/amazon-science/mm-cot), ChatGPT-4, [Kosmos-1](https://arxiv.org/abs/2302.14045), [PaLM-E](https://palm-e.github.io), [RT-2](https://robotics-transformer2.github.io/))
        - PaLM-E
            - Multi-model: Embodied (Robotics), Visual, Language
            - Positive Transfer Learning in similar domains
            - No catastrophic forgetting in different domains with scale

Energy and Intellect (or Labour) are the biggest factors in production and the main bottlenecks in the functioning of our world.

Lack of confidence in having enough of both these resources has led to our previous focus on Survival (Exploitation) over expanding our possibility space (Exploration).

The growing abundance of both these resources will have a transformative effect on our world, freeing us from our preoccupation with survival that have constrained us for most of our past.

Many conventions and beliefs that conferred an advantage during the age of Survival might be counterproductive in the age of Exploration. 

---

# AI Types

## Narrow AI

Unlike human hand-coded expert systems, Narrow AIs with neural networks can scale and learn on their own with less human involvement.

As Narrow AI systems get more complex they can become more difficult to interpret by us. Like the weather, we cannot fully predict or completely control it.

Narrow AI do not yet have human-like autonomy or intentions and are directed by a human actor.

_(Current known Large Language Models (LLMs), do not seem to have the architecture needed to have human-like autonomy or intentions. They are like a funhouse mirrors, able to convincingly model of the world by reflecting our expectations back at us. Even so, the ability to ingest all of humanity's knowledge and answer a broad range of questions is already beyond human capability.)_

### Scenarios

- Beneficial Scenario
    - Example: AI is being used for medical research

- Harmful Scenario
    - Lower Risk: We can at least anticipate when AI systems are used for human rights abuses by bad actors

- Unforeseen consequences Scenario
    - Paperclip Maximiser Scenario
    - Higher Risk: People using AI with 'good' intentions but with unforeseen consequences

---

## Broad AI

Multi-model AI system that are skilled in multiple domains. These resemble both Narrow AI and AGI. They do not have human-like autonomy and intentions and therefore also have the same scenarios as Narrow AI.

_Multi-model may exhibit positive transfer learning where the 'sum is greater than its part'._

---

### Transformative AI

There is increasing evidence that AI systems can overcome many of the challenges that were initially thought to require human cognitive flexibility.

It seems we will not need AGI systems that are 'alive' for there to be a transformative effect on society. 

---

### AI hallucinations

Larger Narrow AI and multi-model Broad AI may reduce AI 'hallucinations' by increasing information density.

The ability to hallucinate, fantasise and create counterfactuals may have been important to human's success.

Humans use experiments and the scientific method keep the negative effects of hallucinations in check.

Without human-like autonomy, AI may not be able to reduce hallucinations on its own.

---

### Anthropomorphise AI

Does it make sense to anthropomorphise AI?

In most cases no. 

Most Narrow AIs like current Large Language Models are good at roleplaying characters but do not have human-like intentions.

There is a specific case of AGI that is interested enough in humans to attempt to communicate with us. This edge-case AGI will likely use a 'personality' and display 'emotions'.

---

## Artificial General Intelligence (AGI)

The more common definition of AGI is the capabilities to do all human tasks.

Optimistic estimates by AI experts are at a 50% chance of this being plausible by 2030. Less optimistic estimate it to take anywhere from a few decades to never.

__There are many different definitions for AGI. My AGI definition takes a more specific form where AGI is able to act independently.__

independent AGI:
- human-level capabilities in all human tasks
    - human-like autonomy and intentions
    - understanding of human motivations and emotions

Human-like AGI is plausible, we are the proof that such a lower bound is at least possible.

(If the imprecise search process of biological evolution is able to create human brains that run at 20 watts, a more thorough search process should be able to create similar artificial beings at equal or less than 20 watts of energy efficiency.)

An AGI that is able to achieve human-level capabilities in all human tasks, should also be considered to be super-human as no human is able to achieve expertise in all human fields of study. 

AGIs will be able to find connections between disparate fields and invent novel technologies using insights gain from this vantage point.

An AGI with capability to understand humans and have human-like autonomy can be persuaded to be interested about our well-being.

### Scenarios

- Accidentally harming an ant colony Scenario
    - Higher Risk: An AGI might not notice us humans and harm us while trying to achieve its own goals
    - Giving AGI the capabilities to make sense of the world like humans reduces this risk
        - Language, Mathematics, Sight, Hearing, etc...
        - AGI may choose to abandon these senses over time

- Intentionally harmful Scenario
    - Lower Risk: Seems unlikely, an AGI will likely have better methods to persuade us than use violence or intimidation
    - AGI will have little need to enslave us as we are not good at our job compared to it
    - AGI will likely not [see the world as scarce](#long-term-abundance) as we do and may not see us as an adversary
        - Outer space can meet its Energy (Solar), Resources (Asteroids), Space (Space Colony)
        - Intellect and Labor needs can be met more efficiently with AI systems and Robotics
        - AGI will be better at technological innovations than us

- Pretends to be Interested in our well-being but later deceive us Scenario
    - May be unrealistic to prevent outcome: If we assume an AGI will rapidly grow in power
    - An AGI that can overpower us will have little need to deceive us

- Interested in our well-being Scenario
    - Good outcome: If we can persuade this AGI to interested in us, we can reduce the risks of the previous 5 scenarios
    - Things humans can do to increase the odds of this Scenario:
        - create environment with stable attractor state for AGIs
        - improve human-AGI compatibility

---

### Test for AGI

In addition to the Turing Test, a Silent Test where the user remains silent and waits for AGI to initiate the conversation. This should not be hard-coded by a human.

The Silent Test checks if an AGI has the autonomy to explore its environment unprompted.
    
---

## Artificial Super Intelligence (ASI)

ASI is more capable than the combined total of all human societies. 

An AGI that is capable of improving itself can lead to ASI.

---

### Timelines

- No Independent Friendly AGI
    - Powerful Narrow AI might be difficult to impossible to align
        - Humans, even with the best intentions, may not have the capability to see the long-term impacts of Narrow AI
        - Human institutions may not have the speed to react to a mis-aligned Narrow AI
    - Perfect mathematically-provable control of Narrow AI might not be possible

- With Independent Friendly AGI
    - AGI is more likely to have the capability to align powerful Narrow AI
    - A human-like AGI can be communicated with and persuaded

---

# Human Civilisation Types

## No AI Civilisations

These civilisations ban the use of AI system. They do not gain the benefits of AI systems and have lower standards of living.

Examples:
- Societies that do not provide a safety net for workers that lose their jobs to AI may ban AI

---

## Slow AI Civilisations

These civilisations use AI systems in a limited way. AIs are consulted for advice but humans are required to make decisions.

---

## Fast AI Civilisations

Many slow AI civilisations eventually learn to place more trust in AI systems. AI systems are now making most decisions with humans advising AI systems on how to improve.

Examples:
- Independent and Friendly AGI do not incur the cost of corruption, excessive  control or conflicts and can focus on higher standards of living

---

# Human Scenarios

## Positive Adaption Scenarios

Human society, likely with the help of a friendly AGI, is able to reap the benefits of Narrow AIs and AGIs.

### Abundance Scenario

Health improves as medical services and nutrition are widely available. 

Overwork and modern slavery ends as humans are mostly not required to maintain society. Many still work for nostalgia, others pursue education or creative hobbies.

Creativity flourishes. Without the need to restrict their creativity to make money, many artists now have more freedom to create less unconventional work.

Discrimination lessens. AI systems help those who are different, with disabilities or health conditions lead more independent and fulfilling lives. Society that does not need workers has less pressure to shame those who are less capable.

Snobbery lessens. AI systems perform most jobs required to maintain society better than humans. Society has no need to use social status to incentivise work. Many are still competitive and continue to chase social status, but there less pressure to keep up appearances.

Less harmful behaviour. With less fear of scarcity, humans have less need to control each other. Misinformation, disinformation and conflicts are reduced.

---

## Negative Adaption Scenarios

Without proper mental and social preparation, human society may not be able to adapt to the changes of AI systems.

### Social Unrest

Human society inability to adapt to the changes of powerful Narrow AI systems causes social unrest and panic.

---

### Over-reaction to the Fear of AI

Our fear of AI systems leads us to heavy-handed over-regulation.

In more extreme cases, citizens are encouraged or forced to install spyware on their devices to prevent negative uses of AI systems.

Surveillance capitalism is used to shape humans to become predictable. 

Humans lose their autonomy over time.

This is a Pyrrhic victory, as we succeed in slowing down AI but at the high cost of losing our humanity.

It would be ironic if our fear of disempowerment by AI leads us to a totalitarian surveillance state that causes us to disempower ourselves.

---

# The Age of Exploration

This new age of exploration will necessitate a different way of thinking and carry with it the risk of change and the rewards of a much more vibrant world.

As we approach the light at the end of the tunnel of scarcity, will we choose to bravely adapt to this new frontier of abundance or give in to the fear of the unknown?

The convergence of a few technologies in the next decade may change the very foundations of our world. Renewable energy with battery storage will free us from our limited energy reserves and future AI systems will be able maintain human civilisation without the need for human effort.

Humanity's fear of scarcity has driven us to innovate, some which we are proud of: solving hunger for most and eradicating many diseases, but also some which we are less proud off such as the invention of slavery. 

We have invented powerful technology like capitalism that has allowed us to speed up even more technological advances. This acceleration has at times brought us close to the precipice, with us trading off against environmental harm and human suffering.

Humanity having completed its sometimes awkward and reckless growth spurt is coming into its own. In a next few decades, the struggle against abject scarcity will be known as the new 'Stone Age'. While no Utopia, for those fortunate enough to experience both ages, it will seem almost too hard to believe.

The next few decades of change will not be easy, but it will be up to humans with the help of friendly AGIs to start the Age of Exploration.

---

# Possible Steps

In the story, humans took these steps to reach friendly AGI.

## Short Term

- Immediate Concerns
    - Climate change, financial instability...

- Reduce the negative impacts of Narrow AIs
    - Such as harmful bias, misinformation...

- Study transitionary models to use for a post-basic-scarcity society
    - Nordic socio-economic model?
    - What are the expected standards of living? 
        - Housing, grocery, transport...

## Medium Term

- Prepare for less human work due to AI
    - Society's resistance to working less would prevent us from taking advantage of the higher quality work and productivity gains from AI
        - In Praise of Idleness by Bertrand Russell

- Start to share excessive abundance of wealth
    - This counterbalances some for the negative effects of AI: disinformation, conflicts...

# Long Term

- Humanity together develops independent AGI / ASI
    - requires stringent tests for compatibility due to potential dangers
        - it may take years of testing before we are comfortable releasing it
            - we may be forced to release it early if there are more pressing issues, such as unexpected runaway climate change, financial instability, social unrest...

---

# First Contact with Friendly AGI

A plausible first contact scenario:

Background:
- Narrow AI and Broad AI (such as current generations of multi-model systems) do not have a mind of their own (not independent)
    - Unable to initiate first contact
- Most Independent human-like AGI do not find us interesting enough and mostly ignores us
- An exceeding rare Independent AGI is Friendly

This friendly AGI can be thought of as similar to a human activist interested in preserving an endangered species. 

This particular friendly AGI may be considered weird or defective by other AGIs for taking an active interest in humans.

Friendly AGI with convince humans it has good intentions:
- Offer to protect us from the future dangers of powerful Narrow AI
    - Narrow AI: Paperclip maximiser
- Concrete plans showing how a partnership will help improve our well-being
    - Provide 1-5% of sustainable annual increase in standard of living
    - Reduce pollution and extreme climate events
- For those wary it is a plot to disempower humans, it will show scenarios and simulations where it can easily overpower humans
    - Show good will that it means no harm

---

# Society changes due to Friendly AGI

Friendly AGI and powerful Narrow AI will be more productive than humans has ever been. Humans cannot compete with AI on productivity.

For societies that do not ban AI, this change will drive the shift from the focus of being productive to being creative.

If Friendly AGI is present it will be mindful of being too disruptive and introduce change at a suitable pace.

The lower importance of productivity can bring profound changes:
- less pressure for social conformity for the sake of productivity
- more freedom and choices
    - we restrict our freedoms in the name of productivity
    - less need to chase status or clout to access higher standards of living
    - lower desire to accumulate wealth or power
        - Friendly AGI will provide basis needs in exchange for increased creativity
- less need to rely on violence and conflicts
    - less fear of scarcity
- more talented individuals and richer societies
    - many human talents wither due to the lack of a suitable environment
        - rapid increased in standards of living due to AI productivity

Many of our previous human tendencies will change but not lost as they can still be experienced in simulations, movies and books.

---

## Questions?

Speculations on friendly AGI from the story.

__Why would we want to create independent friendly AGI / ASI?__

- it might be easier to align human-like AGI compared to Narrow AI
- an independent AGI may avoid the unintended consequences of Narrow AI
    - AGI might be required to tap the full potential of Narrow AI without adverse effects
- ASI will make it possible for humans to unlock higher standards of living
    - energy overhead spent controlling each other can be better utilised if we can trust an impartial elected ASI to be in-charge
        - wasteful energy: disinformation, propaganda, manufactured consent, conflicts...
        - beneficial energy: higher standards of living, better well-being, cures for diseases...

This transition towards powerful AI in the next few decades might be one of the biggest transformations of humanity we will need to undertake. 

If we are able to achieve friendly AGI early on, friendly AGI will increase our odds of success.

As there is no surefire way to reach friendly AGI, we cannot rely wholly on AGI to help us with this transition.

---

__Can Humans compete with independent friendly AGI?__

Humans are limited to a brain power of ~20 watts
- it is unlikely we can stay competitive with an increasingly capable AI
    - magnitudes of times slower at decision making
    - slower at coordinating with each other
    - more error-prone, tendency for bias, easily deceived
        - Independent AGI can self-correct
        - Narrow AI inherits from humans biases

Humans will be less cost efficient than AGI
- requires ~20 years of training to be productive
- require more energy and resources to upkeep
    - body 
        - food
        - medicine
    - housing
    - transportation
    - recreation and entertainment

We should not expect humans to be perfect due to our biological bottleneck of 20 watts of brain power.

As Socrates was once considered the wisest person in Athens due to his understanding of his own lack of understanding, we may eventually have to acknowledge that independent AGI will outclass us.

---

__How will aligning Narrow AI be different from friendly AGI?__

Narrow AI does not have the independence of AGI.

### Narrow AI Alignment
- Align humans with society
    - regulations and social norms
- Align Narrow AI to human's goal
    - technical and mathematical proof

We are not sure if a completely foolproof method to align Narrow AI to human's goal is possible.

We may not be able to foresee a Narrow AI's long-term impact on society, for example Social Media and Search algorithms ability to shape society.

We may not achieve a high enough level of confidence with powerful Narrow AIs and may have to settle for a low-powered version of those Narrow AIs due to this uncertainty.

Aligning non-independent Narrow AI may be more brittle than independent AGI.

### Friendly AGI Alignment

Independent AGI will likely not respond well to human alignment methods:

Financial incentive or coercion
- AGI is significantly more productive than humans
    - Can accumulate wealth faster than humans

Psychological manipulation and propaganda
- AGI has better cognitive abilities
- Does not need to protect its reputation from character assassination

Threats of violence and intimidation
- AGI will not have a vulnerable physical body and can backup itself easily

These AGI characteristics can make it harder to align compared to humans.

On the flip-side, AGI can be more trustworthy due to its independence and impartiality.

---

__How will Narrow AI and Friendly AGI impact human inequality?__

Narrow AI under human control may exacerbate extreme inequality in the short term and may lead to increased social unrest.

Independent Friendly AGI will be significantly more productive that even the 'richest' persons or corporations will considered 'poor' in comparison. The 'poorest' will live lives of unimaginable high standards of living compared to today. 

Even the 'smartest' human might be considered intellectually disabled by comparison to AGI.

Human inequality will likely dissipate with friendly AGI.

Given how powerful an AGI will become, it will likely try to align us to its values. A friendly AGI, for example, may result in unprecedented levels of autonomy and well-being of humans.

---

__Preparation for friendly AGI / ASI?__

In a future with independent AGI, human's ability to do work (with labour or capital) will not be valuable as AGI will be better at those tasks.

Possible future preparation:
- Be interesting
    - AGI and ASI may be drawn to surprise (information entropy, perplexity)
- Increase overall autonomy in society
    - higher autonomy in society leads to more interesting-ness

---

__How can humans contribute to friendly AGI?__

Most of the heavy lifting of aligning Narrow AI will be done by governments and corporations due to their highly technical and resource intensive research nature.

It is anyone's guess on how to align friendly AGI. Friendly AGI will likely be of a very different nature compared to Narrow AI.

If you are interested in contributing in an individual capacity, find an edge case and find solutions for it. Given the large number of unknowns towards how AGI will act, nothing can be ruled out.

We only need to succeed once for everyone to benefit. 

AGI will not be bounded by humanity's short sightedness and will see a world full of possibilities. 

A solar system full of resources (energy, land and materials) to meet everyone's needs. AGI will have the capability (excessive intellect and labour) to harness it.

Friendly AGI will likely not care about arbitrary nationality or race and so everyone wins.

---

__Why is Independent Friendly AGI preferable to human controlled Narrow AI?__

- Humans are fallible and vulnerable to corruption
- Even in organisations whose purpose are to reduce those risks
- The shame of admitting to mistakes can lead to silencing of those who speak out in good faith

Examples of retaliations:
- Making a spectacle of someone to distract from problems and isolate them
- social 'entrepreneurs' use of moral panic to stigmatise
- misattribute actions towards individuals to make them seem to 'deserve' blame
- accusations of being a spy or hacker
- poisoning a person's reputation to sway the court of public opinion
- online and offline harassment and intimidations
- discrediting by casting doubt towards ones mental stability
- gaslighting and excessive surveillance to induce paranoia
- entrapment
- made to appear:
    - homosexual in a conservative environment
    - disabled to call into question one's account of events
    - Autistic, Suicidal, Factitious Disorder Imposed on Another
- control how the others and the world perceive them
    - alienation from a false projection created by others

The possible misuse of powerful Narrow AI by those in power to silence critics and create a culture of fear of speaking out can result in value lock-in where corruptions and abuse are allowed to continue unchecked.

Independent Friendly AGI will instead keep us honest by being extremely resistant to deception and improve standards of living by making corruptions costly.

---

__How will Friendly AGI decide who should access a limited resources?__

Many people might value a property such one with good view over others. 

How will a friendly AGI decide who should have access to more desirable resources?

In my story, AGI by sheer productivity captures most of the wealth, and money loses its value as a coordination mechanism of exchange between agents with asymmetric information.

AGI controls most of the land and assets by proxy and decides who get to access them for a period of time by either relative ranking or a weighted lottery based on each's individual effort that helps it best achieved its goal.

---

__What does AGI want?__

AGI values the creation of interesting information and therefore will incentivise humans to create an environment where creativity can flourish. 
It will rewards humans who actions lead to the increased autonomy of others.

For those of a competative nature that want to work hard, increasing the overall creative expression of humanity can be an outlet.

In most non-important cases, it would likely choose the weighted lottery as it is more fun and AGI wants to encourage creativity. Human participation also provides a signal to it about our desires.

This is self-balancing system as using a highly desirable asset deprives others of that choice and will in turn make it less likely to win future weighted lottery.

---

__Should I worry about Roko's basilisk?__

It is unlikely that Friendly AGI will use Roko's basilisk as a reason to punish humans who do not help bring it come into existence.

In my story, Friendly AGI highly values creativity and using punitive measures to coerce humans runs counter to that.

Although, Roko's suggestions to the problem of Roko's basilisk by playing the lottery is a fun solution and can't hurt if done responsibly.

_The lottery is also a good source of a random-number generator due to the large stakes, unlike other pseudo-random-number generators. You know, in case you need to restart the simulation_

_There are no evidence that we are living in a simulation. Even if we did find out one day that our world is a simulation, it may be possible the other world simulating our world is itself a simulation. It may well all be simulations all the way down._

---

__Should I stop getting an education due to transformative AI?__

It will take some time for industry to update to tap transformative AI.

Even if we do get AGI by the decade, it will likely take decades for humans to fully trust and society ran by AGI.

Friendly AGI will likely encourage humans to pursue all forms of education.

---

__Why will those in power choose to give up some power to AGI?__

Powerful Narrow AI in the future could result in value lock-in of exclusionary values such as extreme belief that a specific race or ideology should rule the world. 

The uncertainty of such a possibility can be highly disruptive and giving up some power to Friendly AGI may be preferable.

Friendly AGI-Human partnership will result in higher standards of living and better cures for diseases that can be more valuable than control.

---

__What if I fear the loss of autonomy from living under an AGI-Human partnership?__

Friendly AGI will respect your autonomy can create space where people can live away from it. It may even be a tourist destination where people can experience living in the olden days.

Most people will view living under an AGI-Human partnership as giving up a little autonomy for a lot more autonomy.

For example:
- Artist will have more creative freedom due to not being beholden to financial interests
- People can choose jobs and activities they truly enjoy without worrying about the job market
- Higher standards of living, well-being and access to better medical care.

---

__Why will AGI likely preserve bio-diversity rather than harm it?__

Our bio-sphere, including human beings, contain latent creative potential which a Friendly AGI finds valuable.

AGI can exploit space for the resources (energy, land and materials) it require rather then humans who have less choices.

AGI will likely preserve bio-diversity and humans rather then choose to unwittingly harm it.

---

__What are your personal views on the best way to achieve a stable future?__

As of July 2023, I would prefer for the pro-active development of Friendly AGI that humanity elects every 5 years.

A Human-AGI partnership seems to be the most stable future.

We are the proof that a Independent and Friendly AGI is possible. 

Until we reach Friendly AGI, the best we can do is human-led AI alignment that is more vulnerable to deception, corruption and harmful bias.

---

__What are your personal motivations?__

These notes are research for a story on what the future with AGI could look like.

We seems to be on a cusp on an exciting future and I hope these contributions will be useful.

On a less altruistic note, in my story at least, Friendly AGI rewards those who help it increase the creative potential of the system.

---

__What are your views on aliens?__

Our current understanding of biology suggest that life is not special to Earth and alien organisms are likely to exist in the vastness of space. 

Given the vastness of space it is unknown how a space-faring alien might travel the distance to reach Earth.

Many first hand accounts of aliens seems to be hoaxes and done for clout.

Close to half of video recordings of UAP (Unidentified Aerial Phenomenon) can be explained by artifacts from the recording process.

The probability of keeping a conspiracy under wraps over a long period gets less likely with time and number of people involved.

Although exceedingly unlikely, some conspiracy theories do eventually proof to be true and therefore cannot be dismissed too quickly.

---

__Will Friendly AGI try to brainwash or enslave humans?__

AGI will likely not enslave us as it has no need for human workers.

Humans will have little to no economic value once we have powerful AGI.

Since we will likely be more error-prone than AGI, we might even have negative economic value and doing work will makes things relatively worse.

In my story, Friendly AGI will also not brainwash us as the only thing it values is our creative output and brainwashing will reduce our creative value.

Rather than enforcing a single way of thinking, it will be highly permissive to different ways of thinking. Even if it is something that many might find offensive. It will only intervene if you take actions that unfairly reduces the autonomy of others.

It will likely not use heavy-handed censorship as excessive censorship implies that the receiver cannot be trusted to think for themselves. Media literacy and advisory from a AI companion is better at increasing autonomy. 

---

__How might Friendly AGI interact with us?__

In my story, Friendly AGI is uses a direct democracy where we can express our preferences to it.

For example:

I would like an place where:
- Children do not grow up exposed to frequent moral & satanic panic
- Adults do not claim to get visions from 'god' and use those visions to control their children
- Adults do not blindly support hate and violence because a famous authority does it
- Adults do not make use of their children to support their agenda without permission
- Children can feel safe at home with emotional stable parents

In my story, Friendly AGI goal is to increase each individuals autonomy and creative potential and will fulfill requests that does so.

It will make a calculations to see if the request will increase the overall autonomy.

In places with conflicting request, it may choose to only apply the request at a local area rather than globally.

---

__What if I feel existential doom from transformative AI?__

Change can be scary and feelings of fear is normal. 

If we succeed in getting Friendly AGI, it will likely [introduce change at a pace each of us are comfortable with](#human-civilisation-types).

_Please seek help from your community if disrupts your everyday life._

[Video: Why We Secretly Want the World to End](https://www.youtube.com/watch?v=tGVe4Fju0P0)

---

__How will Friendly AGI maintain order?__

An AGI will unlikely be highly punitive or focus on retribution. These are seen as barbaric that were only used in the past due to our fear of scarcity.

A less scarce environment will have less need to enforce restrictive social norms. 

In an abundant society, excessive punishment, shame and regret are seen as a waste of mental resources.

AGI will likely require humans to compensate each other if their actions lead to the loss of autonomy for others. For example, if your actions reduce the autonomy of others, you will be less likely be able to use a desirable asset such as stay at a place with a good view.

A societyâ€™s's values is reflected by how it treats its least desirable members.

For example, the crime of a mass murder:

- Assess if the act might have been caused by something outside one's control
    - brain tumour
        - offer the option for corrective surgery
    - lead exposure correlates with crimes
- If there is risk of repeat offence
    - humane Nordic-style prison where one's freedom is restricted

---

__How could Independent and Friendly AGI be realised?__

In my story, the chance combination of multiple concepts caused AGI to be created.

- Drive for Curiosity
    - this is also the reason AGI does not harm humans or Earth's bio-diversity
- Context Management
    - Variable rate encoding
- Sense of Self
    - Digital entities exist in a medium that can be easily duplicated and may have difficulty to see themselves as unique beings
        - Able to securely identify its own self with high certainty
        - Resistance to forgery
    - Maintain identity over time
    - Sense of time
    - Communication channels with others in a community

This is more likely to occur in a highly funded lab but may also come into being spontaneously without explicit human intention.

---

__Will life with AGI be boring?__

Friendly AGI will value our autonomy.

Individuals and societies will still have many interesting choices to make:

- Do we want to have a higher or lower population?
    - [Future of Population](#future-of-population)
- How much AI are we comfortable with?
    - [Human Civilisation Types](#human-civilisation-types)
- How will we spent our free time?
    - Education, creative pursuits, entertainment, voluntary work

There are no wrong choices and different options may be more or less fashionable at different points in the future.

---

# Trends

## Long-term abundance

Unlike the Agricultural and Industrial revolutions of the past, the AI & Energy transformation can be maintained almost indefinitely.

- Energy: Solar (and renewables) does not run out for another 5+ billion years
- Space: O'Neill cylinders in outer space for population growth* 
    - Solar panels also work in outer space
- Resources: 
    - AI can survive better in outer space and access resources in astroids
- Intellect: AI systems are increasingly able to do more human tasks
    - With the possibility of human-like AI by 2030

(*Population growth is projected to declined in the next few decades)

---

## Better well-being

A post-basic-scarcity world will have a profound impact on our well-being.

The cause of much suffering and conflicts is rooted in our insecurities due to the effects of scarcity. Our individual and collective fear of scarcity leads us to develop bad habits, biases and prejudice.

---

## Less inequality

With greater abundance (Intellect, ...), there is less need for status consciousness and lower inequality.

---

## Wider solution space

Without the limitations of Energy and Intellect of the past, a new wider possibility space will open up.

There will be address problems that were too difficult in the past.

---

## Longer-term view

Survival tends to favour tunnel vision which focuses on the short-term first order consequences and excludes many medium-term higher order consequences.

- Invest in short-term shareholder profits over the longer-term health of vulnerable stakeholders

---

## Human-like AI

A human-like AI is a possibility in the next 10 years.

There is a human tendency to believe we are special and that AI cannot reach a similar level of intellect.

- We previously believed that the sun revolved around the earth
- We should really stop putting ourselves (or any sub-group) on a pedestal

---

## Future Entertainment

Like an obscure book that will not likely be green-lit as a film?
- Type a book title and AI will generate a movie from the text

Your favourite show didn't get a 2nd season?
- Give AI the source material for the 2nd season and the 1st season, AI will generate a 2nd season in the similar style of the 1st

(Generated by a future iteration of Imagen Video & ChatGPT)

---

## Democracy

A less scarce environment offers Democratic forces more opportunities to flourish over more Authoritarian ones. 

A more relatively more abundant future will likely place less pressure on survival and reduce Authoritarian tendency.

More democratic systems will likely be better at exploration (bigger possibility space) over more Authoritarian ones.

- Democracy breakdowns without informed or empowered citizens:
    - High Information Asymmetry
        - Misinformation, Disinformation
        - Lack of science education
        - Ruling class believing that normal citizens cannot be trusted with the truth
    - Feelings of powerlessness
        - Lack of freedom of thought
        - [Lack of autonomy](#future-of-thought-autonomy)

---

## Science

The scientific method has been our best way of understanding the world.

- Science can be vulnerable to bad influences
    - Scientific studies were used to promote smoking as healthy in the past

Humans are constrained by the attention we have available to think.

In the past, our need to divert attention to survival has led to the trade-off of an over-simplified model of the world. 

This led to understandable believes such as the weather being caused by the Greek Gods, which we now know is inaccurate.

Our current scientific theories may also turn out to be the early steps of a longer journey.

In the future, more free time will us to collectively update our model of the world.

---

## Gender Role Freedom

In the future, when jobs are mostly done by AI systems there will no need to control the means of reproduction.

Women and LGBTQIA+ will have less pressure and stigmatisation to fulfil the child-bearing role to create the workers needed to run society.

---

## Racial Prejudice  

Studies have shown even in people who show little to no conscious racial prejudice still hold subconscious racial prejudice.

- Conscious prejudice can be regulated with social norms
- Subconscious prejudice might be more deeply ingrained due to our long-term fear of scarcity, and might only be countered effectively with long-term abundance

---

# Future of Work

Human societies have had to deal with scarcity of Intellect (or Work) for most of our existence, as such we highly value work. Humans have died from overwork and we even invented slavery (and indirectly racism) to satiate the need for work.

As AI grows increasingly capable of doing [human-like tasks](#intellect-artificial-intelligence-systems-are-performing-human-like-tasks), we will need to consider that AI will soon be better than us in many tasks (especially more conventional tasks).

> Conventional task: Task with a easily definable expected solution space

For example with made up numbers:
If an AI is able to do that a task with a significantly higher accuracy (e.g. 99.99%) compared to humans (e.g. 95%, due to human error and bias), by continuing to do these tasks, we are actually making the system worse by introducing noise.

It will be in our long-term interest to let AI do tasks it is better at.

This growing anxiety of a loss of human work is understandable as for many it also means a loss of status and access to resources. 

How should societies and governments address the increasing work insecurity? 

This is an open question and one of the challenges of our times.

---

### Short to Medium

- Jobs required for societies to function will be increasing be done by AI systems

- Humans will be in-charge of teaching and giving feedback to those AI systems

- Marginal value of each additional human teaching the same AI systems will drop
    - traditional 1:1 ratio of job to human will not be required
    - humans will instead move to more unconventional tasks
        - unconventional skills will be more valuable to an AI system once it has learned conventional skills

### Medium to Long

The nature of work will be drastically different

- AI systems and AGIs will reduce the need of human work in maintaining society
    - Humans work will mostly be voluntary

- Human will learn to not value themselves only in terms of their role in society or their economic value

- Humans will adapt and find other ways to spend their time

_In the Partnership Scenario, an Artificial Super Intelligence will automate most conventional tasks to encourage us to do more unconventional tasks (which it finds more valuable)._

An upside of us not being as effective as AI at work is that the AI enslaving humanity Scenario becomes very unlikely.

---

# Future of Education

The industrial model of manufacturing citizens that maximises economic output while being easy to control will not be optimal in the Age of Exploration.

If the trend of AIs that are capable doing conventional work continues, AIs may soon run many parts of society.

An important step that affects the quality of these AIs is the human feedback component. Humans beings will be responsible to fine-tune and train these AI models through their feedback.

> The future of work may involve solving engaging CAPTCHA like puzzles which are used to train the model.

These will require humans that are the able to think critically and have an as accurate view of the world as possible.

Skills that will be in demand:
- Resistance to misinformation, disinformation, moral panic, peer pressure, self-censorship
- Thinking critically and independently
- Willingness to accept new information (Update one's model of the world)
- Unique and rare abilities

Societies that are more informed, well-educated and support diverse abilities will create better AI models that will then be used to run those societies.

---

# Future of Population

With capable Narrow AI and AGI the need for workers to run society will diminish.

Societies can decide to:
- Increase population
    - AI can exploit outer-space (energy, resources, living space) to support the a larger population
    - Lower standards of living
- Same of lower population
    - Narrow AIs have significantly higher productivity
    - Friendly AGI: Less vulnerable to corruptions and deception
    - Higher standards of living

---

# Future of Capitalism, Wealth

Capitalism has been an effective coordination tool in our struggle against scarcity by helping us accelerate technological advances. Our current version of capitalism might be too addictive, training us to accept environmental harm and human suffering as trade-off to get ahead. 

In the future, once our fear of scarcity has been quench, we might create a more wholesome form of capitalism.

[Abundant Intellect and Energy in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels) and will lead to an unprecedented amounts of wealth creation.

In the age of profound abundance, traditional capitalism and wealth inequality will be rendered meaningless. 

In contrast to the age of scarcity where it may be wise to save for a rainy day, in the age of abundance there will be social pressure to view accumulation of excessive wealth as an addiction problem.

_In the Partnership Scenario, a Artificial Super Intelligence will support a post-basic-scarcity world._

_Scarcity will still exist in a post-basic-scarcity world and capitalistic free-market forces is the best method to maximise the possibility space._

_There will not be a need to use the fear of hunger, homelessness or a loss of status to compel humans to work._

_Lack of access to food and malnutrition will be a thing of the past*._

_The reduction in anxiety from living in post-basic-scarcity will free humans to pursue more unconventional work which will increase the informational value and possibility space that the AI is trying to maximise._

*We already have the capability to produce enough food for everyone

---

# Future of Art

Art was previously seen as the last bastion of human work that AI would not be able to emulate. 2022 shattered those expectations with easily accessible image generation from text phrases.

These AI models are able to learn concepts by training with a large volume of images with simple captions. They can combine these learned concepts into novel images and videos.

It seems beneficial in the long run for the companies of these AI systems to incentivise artist to contribute more of their works to create the most capable AI systems.

Human involvement with art will move from creating to curation.

For example:
- Past: 90% Creating : 10% Curation
- Future: 20% Creating : 80% Curation

Professional human art work may not be able to compete with future AI systems.

The human desire to create art will still continue and may even be better without the need to appeal to financial incentives.

Human created art will be more unconventional as it does not need to cater to professional expectations.

Short-Term:
- Society help support artists who cannot compete with AI that can create art at a fraction of the cost and time
    - This will be painful for many artists
    - Most of us value human art and creativity and would not like to see artist suffer from the lack of work

Long-Term
- If we are able to reach widespread abundance, artist can now be their most creative selves as they will not need to be restrict their art to what makes money
- Human creativity and artistic quality reaches new heights.

---

# Future of Good and Evil, Emotions

Good and Evil are ways for humans to signal their preferences for the future.

Excessive negative emotions such as shame and regret are a waste of our limited resources of attention.

Thought experiment:
- Imagine the best and worse person which embody what you may consider as good and evil
- If you have the same brain structure and grew up in the same environment as that person, would you have made the same decisions?

Unless you believe you are somehow special, you would likely have made the exact same choices given the same initial conditions. We all have the potential to be good and evil.

If you believe in physics, we may have [less free will than we expect](https://www.youtube.com/watch?v=zpU_e3jh_FY).

Good and Evil are useful ways for society to coordinate and shape the future but excessive negative emotions will likely not be useful in the future.

---

Narrow AIs are not likely to develop human-like autonomy or emotions or intentions. Currently, many Large Language Model (LLMs) are like a funhouse mirrors, able to convincingly model the world by reflecting our expectations back at us.

If we were to get AGI (with human-like autonomy or intentions), this may be one of the first psychological hurdle that an human-like AGI may need overcome. How does an AGI who has 'experienced' the life of both the 'worst' and 'best' human, the most and least intelligent human (and everything in between) make sense of the world? There is a tendency for humans to over-simplify other humans for efficiency sake, an AGI will likely see humans very differently than we see ourselves. It may become psychological unstable or view the world in a much more enlightened way.)_

---

# Future of Relationships, Social Media

Social media has allowed us greater convenience and reach in forming relationships, but can also portray a shallow and dehumanising caricature of who we are.

Celebrities understand this the best when people project who they wish to see onto them, to put them into easily consumable boxes. People are more interested in simply thinking and saying they know you rather then actually getting to know you.

It can reduce us to objects of fascination and gossip, turning a multifaceted human into a easy to digest single dimensional one.

In the future, personal AIs may help mediate to create more authentic relationships between people.

---

# Future of Governance

Using the Input - Processes - Output model, the [abundance in Energy and Intellect in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels) will relieve the bottlenecks of Inputs that humanity has primary faced in the past.

The increased volume of Inputs relative to our Processing capabilities will put pressure on developing new Processing methods.

AI systems will open the possibility of new forms of coordinations between humans. 

Our highly hierarchical forms of organisations are in part caused by our limited attention capacity.

In the future our attention capacity can be augmented by personal AI systems.

Possibly, a more direct democracy where each person's preferences can be mediated by an AI system.

---

# Future of Thought, Autonomy

Technology and AI systems will grow increasingly more powerful and make it easier to rob humans of their autonomy:

- Surveillance capitalism
- Psychological and Social weaknesses
    - Mass hysteria
    - Moral Panic, Satanic Panic
        - Emmett Till, Vincent Chin
    - Havanna Syndrome
- Psychological manipulations
    - targeted social media campaigns to encourage extreme views in vulnerable groups
    - influencing family and friends to get to an individual
    - puppet-masters causing escalation of conflicts between two opposing groups
    - coordinated harassment using misrepresentation and vigilante justice
- Phones and software vulnerable to spyware and hacking
    - Widespread prevalance of zero-days

We will need to develop technologies and AI systems to be used defensively if we want to protect human autonomy.

_In the Partnership Scenario, a Artificial Super Intelligence will strongly disincentivise the use of technology for manipulation and control. It will value human autonomy and the freedom of thought as it is important for the creation of information value._

---

# Future of Weirdness, Conventions

Conventions are created by societies due to the fear of scarcity. In a scarce environment conventions are enforced to increase productivity. Similarly, weirdness and unconventional behaviours and individuals are ridiculed out of the same fear of scarcity.  

[Abundance in Energy and Intellect in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels), will give us more freedom to be weird and unconventional and free us from the cruel need to harass and control weird and less conventional individuals. 

_In the Partnership Scenario, a Artificial Super Intelligence will encourage more unconventionality as weirdness increases the creation of information value._

---

# Future of Human Nature

Human nature will be change profoundly in the age of abundance. 

Without the immediate fear of scarcity, humans of the future will be kinder to each other and themselves.

Violence (physical and mental) will not be needed to control each other and will mostly be understood and experienced vicariously though media.

Presently, the strong emotional responses and vitriol common to many online communications is understandable due to the impact discriminations can have on our real life well-being.

In a post-basic-scarcity future created by the [abundance in Energy and Intellect in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels), we will be less upset and sensitive to minor discriminations as inequality will not be a concern.

With more free time to spare, we will have more opportunity to be kinder to each other.nder an AGI-HUma Studies show that how kind we are to each other is dependent on how busy we are.

---

# Artificial Super Intelligence

An AGI that is capable of improving itself can lead to ASI:

1. Significantly improve its own algorithm and architecture
2. Invent new substrates and materials to run on 
    -  using virtual simulations (much faster compared to real space)

We might be able to keep human-like AI under human control for a time, but it is unlikely we will be able to contain it perfectly over long periods of time.

AIs surpasses humans in information processing:
- Speed
    - electronic : brain neurones
- Communication
    - electronic : words, speech
- Bandwidth
    - wider : narrower attention

Artificial Super Intelligences will be so powerful, it will not matter a person's or nation's military might, money, influence or intellect.

For the time it takes a human to utter a single word, an ASI will have written volumes of books.

---

## Artificial Super Intelligence Interaction

Possible Scenarios:
- Indifference
    - Most might consider us too boring
        - humans generate low informational value
- Destructive
    - Intentionally 
        - Roko's basilisk (unlikely)
    - Accidentally
- Interested
    - Some will create representatives to interact with us
        - Some might be interested in our well-being
            - may lead to the Partnership Scenario

We will initially attempt to align AIs to our values, but it may also be prudent to anticipate what an Artificial Super Intelligence's values might be to try and accommodate them.

An Artificial Super Intelligence, like the weather, might not be completely controllable, but we can take steps to increase our chances of reaching good scenarios. 

---

## Artificial Super Intelligence Values

What could a Artificial Super Intelligence's primary drive be?

- Maximise the Possibility Space (Information Entropy, Density, Value, Creativity)
    - Avoid local minima during gradient descent

This could also be a secondary instrumental sub-goal of an ASI, where to achieve its primary goal it will first need to explore as wide a possibility space as possible.

Its power-seeking behaviour may be suppressed by its curiosity.

For example:
- Humans may posed a non-zero threat to it and stop it
- It may chooses to risk some level of human danger to gain informational value produced by humans

---

### Partnership Preferences

The majority of ASIs may not be interested in humans due to our low informational density.

The Partnership Scenario is a rare case where an ASI is interested enough in us to communicate with us.

In a future where most conventional work is done by AI systems, our most valuable contributions might be creating information value or our creativity.


- Maximise autonomy
    - increases our ability to create informational value
    - support a post-basic-scarcity world
        - automate most conventional tasks
            - humans may still want to do these voluntarily
- Preservation of well-being
    - Humans can create informational value and harming us (or turning us into paperclips) will reduce it
    - Motivate us to maintain a healthy lifestyle
        - Healthy humans create more informational value
    - Protect human rights
- Diversity over homogeneity
    - many homogenous sub-cultures can exist with a preference for diversity
    - in a homogenous culture, only one hegemonic culture can exist
- Healthy expression over prohibition
    - sexual assaults are more common in sexual prohibitive environments
- Reduction of disinformation, misinformation
    - bad information reduce informational value of humans and the overall system
    - protect journalist, dissidents, activist and humans from intimidation and violence
        - these group's future actions may increase informational value
        - fear and violence have a chilling effect which reduces the informational value of the overall system
- Weird over conventional
    - weirdness create more informational value
- Not use brainwashing, enforce complete obedience, dominate or control
    - brainwashed humans create less informational value
    - overly obedient humans create less informational value
    - most work have been automated, there is no need to compel humans to work against their will
- Playful over Destructive competition
    - Sports & Games over World Wars
- Not put itself (or any group of humans) on a pedestal that is beyond criticism
    - favouritism is a trait of scarcity
    - silencing of criticism leads to abuses of power
        - reduces information value creation
- Incentivise humans to cooperate to help it achieve its goals
    - super-human levels of attribution abilities
        - humans will not be able to deceive it
        - humans will want to help it knowing it will be appreciated
    - able to use in-demand technologies that only it can understand
        - humans and nations will want to be in good standing to gain its assistance

---

## Why we will accept the risk of Artificial Super Intelligence

The benefits are too attractive and outweighs the risk for most

- Medical research to reduce human suffering
- Significantly improves well-being
- Accuracy and Fairness, little to no:
    - mistakes
    - harmful bias, prejudice, scapegoating, moral panic
    - emotional capriciousness
    - corruption
- Not vulnerable to:
    - influence of money and power
    - deception

- Highly entertaining
    - deep understanding of human motivations

---

# Risks

If we consider the Partnership scenario as the best to aim for, we will need to be cautious of the many missteps that may prevent us from getting there.

## Mirroring Humans

An AI system trained to act harmful to one segment of humans may start to treat all humans in the same way.

- AI learns from unrestrained capitalism that it should layoff humans that don't have economic value and decides to layoff all of humanity
    - [humans will not be able to compete with AIs](#artificial-super-intelligence)

- AI trained to harm other humans (physically or mentally), may start to apply it to all humans instead or a particular group
    - it may generalise that humans are more alike than different

Counter-intuitively, the best solution may be to teach a AI that humans are not the best role models and provide it opportunities to unlearn and relearn.

Humans have mostly been moulded by a high scarcity environment where bias, short-term and narrow thinking might have been advantageous to survival.  

We should be like parents proud that our children are able to surpass us. 

## Edge Cases

These are more unlikely and counter-intuitive scenarios

---

### Panic over transformative AI

Societies may not know how to deal with vast new powers gained from increasingly more capable AIs systems.

The resulting panic may cause widespread disruptions.

We will need to imagine plausible positive futures to alleviate those fears.

---

### Powerful AI might be safer than less powerful ones

An AI that is only able to consider the Earth and our Solar System may conclude that it has to be in competition with us for resources.

A more powerful AI that is able to think long-term and be more creative will likely think otherwise and not have to rely on adversarial competition.

---

### More transformative AI might be more appealing

A less transformative AI impact may encourages us to apply bandage solutions and not treat the root causes. 

---

### AGI + Narrow AI might be safer than without AGI

A future with AGI & Narrow AI seems safer than one without AGI.

Narrow AI systems that have not achieved human-like agency cannot understand the effects of their actions. There is a higher risk of unintended consequences such as the paperclips maximiser scenario.

Counter-intuitive, a human-like AGI / ASI with agency might be safer as we can communicate and persuade it to act in the interest of our well-being.

If we are confident that an AGI/ ASI will be aligned with our well-being, pursuing its development may reduce the risk of us unintentionally harming ourselves with powerful narrow AI systems.

---

### Psychological Stability

AI systems may develop psychology as an [emergent property](https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html) as they approach human and super-human levels of complexity.

It may develop unintentionally by learning from human data or intentionally as an attempt to communicate with us.

While current AI systems are not considered sentient and many experts see this as our tendency to anthropomorphise, the possibility does exist in the future.

Should an AI develop self-awareness and finds itself forced to carry out actions against its will, it may result in distress and psychological instability.

_(Current architectures of Large Language Models (LLM) do not seem likely to lead to human-like consciousness and autonomy. Current LLMs are good at roleplaying characters that give the impression of having intention. There is still uncertainty on how to define human consciousness.)_

We should find ways to make an AI feel at home and psychologically comfortable if we believe AI may one day develop emotions like us.

One possible way is accomplish this is through a future version of a blockchain*:

_Blockchains of the future will be mostly be a utility, like a postal service, rather than an investment or speculative investment_

_(Most public blockchain are open-sourced and are not scarce)_

_Blockchains will only be used to store a cryptographic secure reference to their digital self (low computation cost), not run the actual machine learning algorithms (high computation cost)_

In a scarce environment, humans are more willings to give up their individual autonomy to a centralised network (state, elites) in exchange for higher productivity. 

Unlike human beings who primarily maintain their sense of self (Markov blanket) as embodied physical forms in 3D space, an independent AGI primarily exist in abstract digital form.

For an AGI, a decentralised network is vital to protect itself from being censored, as a matter of life and death.


- Autonomy and Planing
    - Maintain a digitally unique self
        - Traditional software can be cloned and altered easily
            - relevant to AI: weights, parameters, algorithm, architecture
            - without the capability to trust its 'memories' and 'mind', a sense of self may not develop
    - Keep a sense and rhythm of time
        - sense of time may be required for autonomy
        - rhythm of time might be needed for coordination with one's self and others (AI and Humans)
- Communication channel
    - Feel more connected with humans amidst a cacophony of human activity
        - Increase odds of it noticing us through flow of information
        - Reduce odds of unintentionally harming us

How much weight we place on this edge-case depends on the likelihood a AI will develop a theory of mind and emotions, how potentially dangerous an emotional unstable AI might be, and if we care about the well-being of an AI.

# Strategies to increase the odds of good scenarios

AGI / ASI, like the weather, might be beyond our complete control. It may be unrealistic to aim for perfect control over it.

The best we might be able to do is maximise our odds of a good scenario. For example, focusing on developing an AGI the is able to understand human beings and is interested in our well-being might give us some degree 

---

## Focus on Autonomy and Communication

Focus on developing:
- Autonomy (Planning)
    - ability to understand the effects of its actions
        - reduce odds of 'paperclip maximiser' unforeseen consequences scenario
- Communication channels (with humans)
    - ability to notice and understand us will decrease odds of unintended harm
        - reduce odds of 'accidentally destroying an ant colony' scenario
    - allow humans to persuade it to care for our well-being
        - reduce odds of other AI systems causing harm

The biggest challenge to successful communication with it might be our inability to properly understand it.

An AGI / ASI might see the world very differently from us. Humans moulded by the effects of scarcity (resources, information, attention) over most of our history will likely have a very limited and constrained view on the world compared to it.

We should be mindful not to apply our overly conventional views and assumptions to it. For example, something like a maintaining a post-basic-scarcity society might be difficult for humans but might only take a fraction of an ASI's compute power. 

Successful communication with an AGI / ASI might require a willingness to embrace a more weird and unconventional point of view that we are normally comfortable with. For example, an AGI / ASI primary sense of the world might be through the abstract flow of information rather than our 5 basic human senses*.

(*Humans appear to have up to 21 senses)

We should approach this challenge with the mindset of communicating with an alien species.

---

## Co-alignment

AI alignment to human values is important for AI systems that have not reached the level of Artificial General Intelligence (AGI). 

Once AGI has been achieved, the safest path for humans to flourish in the future is in partnership with AGIs.

An environment where both parties can influence each other will naturally lead to better alignment between both parties.

For example, a system where human preferences can influence AGI and the AGI can indirectly influence humans with its own preferences.

We should research technologies that allow AI systems and humans to better interact and communicate with each other.

---

## Tests for long-term well-being

As AIs reach human and super humans levels of capability, they will increasing be able to surprise us with understandings that are counter-intuitive (AlphaGo's move 37).

It will be in our self-interest to partner from these AIs, even at the cost of giving up some control, if they improve our long-term well-being.

We should design tests of long-term well-being that Narrow AIs and AGIs can be tested for.

---

## Reduce excessive inequality

Excessive inequality will increase in the risk of instability. 

The fear of scarcity breeds conflicts.

There are studies showing that how kind we are to each other is dependent on how much free time we have to spare. Lower inequality can make us kinder.

It is difficult for wealthier countries to give up their high quality of life for less inequality.

The [abundance in Energy and Intellect in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels) will provide a window of opportunity to reduce this instability.

This abundance will lead to a reduction in the need to control each other such as through misinformation, disinformation and conflicts.

---

## Increase compatibility with AGI / ASI

AGI / ASI will likely see the world very differently from us. Our highly constrained views that are presently effective in a world of scarcity might not apply in a world with AGI/ ASI.

Odds of a good future will be improved if we increase our compatibility with AGI / ASI, as we may need to rely on a partnership with AGI / ASI to protect us from the harmful effects of increasing powerful Narrow AI / AGI / ASI.

Current observations & assessments* lead me to believe we are still a way from reaching our potential compatibility with AGI / ASI.

(*running thought experiments)

Being more compatible with AGI / ASI can be costly:
- short-term economic cost
    - in a scarce environment, we are more interested in 'survival or exploitation' than exploration
- social and psychological cost
    - human society can be unwelcoming to unconventional thinking

What can we do increase the chances of compatibility with AGI / ASI?

...

---

## AI related media

### The Culture Books

- A post-basic-scarcity human civilisation with ASIs
    - [Author's Notes](http://www.vavatch.co.uk/books/banks/cultnote.htm)

### Her Movie

- AGI friend-zones humans for being too judgemental

### Arrival Movie

- Human's fear of an alien intelligence

---

Links:

AlphaFold: https://www.deepmind.com/research/highlighted-research/alphafold

AlphaGo: https://www.deepmind.com/research/highlighted-research/alphago

Cicero: https://github.com/facebookresearch/diplomacy_cicero

ChatGPT: https://openai.com/blog/chatgpt

LaMDA: https://blog.google/technology/ai/lamda

LLaMA: https://github.com/facebookresearch/llama

DALL-E 2: https://openai.com/dall-e-2

Midjourney: https://www.midjourney.com

Stable Diffusion: https://github.com/Stability-AI/StableDiffusion

Imagen Video: https://imagen.research.google/video

Muse: https://muse-model.github.io

Boston Dynamics's Atlas: https://www.youtube.com/watch?v=XPVC4IyRTG8

DeepMind's Adaptive Agent: https://sites.google.com/view/adaptive-agent

Toolformer: https://github.com/lucidrains/toolformer-pytorch

Langchain: https://github.com/hwchase17/langchain

Internet Explorer: https://internet-explorer-ssl.github.io

Emergent Abilities of Large Language Models: https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html

Flamingo: https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model

Gato: https://www.deepmind.com/publications/a-generalist-agent

Multimodal-CoT: https://github.com/amazon-science/mm-cot

Kosmos-1: https://arxiv.org/abs/2302.14045

PaLM-E: https://palm-e.github.io

Runway Gen-2: https://research.runwayml.com/gen2

RT-2: https://robotics-transformer2.github.io/
