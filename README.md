# AI Futures: The Age of Exploration

_Rough sketches of our possible AI futures_

- _There are many possible futures as various Sci-Fi books have shown_
- _If you are reading this a few years from the posting date, disregard it. My thoughts on the matter may have changed._
- _Not professional advice, but a story of a possible positive future where AI co-exist with humans_

---

### Summary:

- The world will increasing shift towards possibility space expansion (explore) from the previous dominant mode of survival (exploit) due to the abundance in Intellect and Energy in the coming decade

- Maximising Possibility Space (Entropy) as a solution to the AI alignment problem

---

The central tension in the development of AI is the balance between **Exploration** and **Exploitation**.

- The Exploration-Exploitation Trade-off:

    All organism (artificial or otherwise) at each time-step must decide to either update their understanding of the world (**Explore**) or survive (**Exploit**). Successful AI systems learn the optimal strategy of when to make either choices.

The increasing abundance of both energy and intellect in the coming decade would shift us towards greater freedoms (**Exploration**) from a focus on survival (**Exploitation**).

- ### Energy: Solar (and other renewables) compared to our limited reserve of fossil fuels

- ### Intellect: Artificial Intelligence systems are performing human-like tasks:
    - Medical Research ([AlphaFold](https://www.deepmind.com/research/highlighted-research/alphafold))
    - Game of Go ([AlphaGo](https://www.deepmind.com/research/highlighted-research/alphago))
    - Game of Diplomacy, Strategy, Human Cooperation ([Cicero](https://github.com/facebookresearch/diplomacy_cicero))
    - Dialogue Simulation ([ChatGPT](https://openai.com/blog/chatgpt), [LaMDA](https://blog.google/technology/ai/lamda), [LLaMA](https://github.com/facebookresearch/llama))
    - Image and Video Generation from text descriptions ([DALL-E 2](https://openai.com/dall-e-2), [Midjourney](https://www.midjourney.com), [Stable Diffusion](https://github.com/Stability-AI/StableDiffusion), [Imagen Video](https://imagen.research.google/video), [Muse](https://muse-model.github.io))
    - Real time Decision and Planning, Robotics ([Atlas](https://www.youtube.com/watch?v=XPVC4IyRTG8))
    - Real time Learning ([Adaptive Agent](https://sites.google.com/view/adaptive-agent))
    - Real world Learning (Toolformer, [Internet Explorer](https://internet-explorer-ssl.github.io))
    - Some Generalisation ability, Multi-model ([Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model), [Cato](https://www.deepmind.com/publications/a-generalist-agent), [Multimodal-CoT](https://github.com/amazon-science/mm-cot), [Kosmos-1](https://arxiv.org/abs/2302.14045), [PaLM-E](https://palm-e.github.io))

Energy and Intellect (or Labour) are the biggest factors in production and the main bottlenecks in the functioning of our world.

Lack of confidence in having enough of both these resources has led to our previous focus on Survival (Exploitation) over expanding our possibility space (Exploration).

The growing abundance of both these resources will have a transformative effect on our world, freeing us from our preoccupation with survival that have constrained us for most of our past.

Many conventions and beliefs that conferred an advantage during the age of Survival might be counterproductive in the age of Exploration. 

---

# Trends

## Long-term abundance

Unlike the Agricultural and Industrial revolutions of the past, the AI & Energy transformation can be maintained almost indefinitely.

- Energy: Solar (and renewables) does not run out for another 5+ billion years
- Space: O'Neill cylinders in outer space for population growth* 
    - Solar panels also work in outer space
- Intellect: AI systems are increasingly able to do more human tasks
    - With the possibility of human-like AI by 2030
    - AI can survive better in outer space and access resources in astroids

(*Population growth is projected to declined in the next few decades)

---

## Better well-being

A post-basic-scarcity world will have a profound impact on our well-being.

The cause of much suffering and conflicts is rooted in our insecurities due to the effects of scarcity. Our individual and collective fear of scarcity leads us to develop bad habits, biases and prejudice.

---

## Less inequality

With greater abundance (Intellect, ...), there is less need for status consciousness and lower inequality.

---

## Wider solution space

Without the limitations of Energy and Intellect of the past, a new wider possibility space will open up.

There will be address problems that were too difficult in the past.

---

## Longer-term view

Survival tends to favour tunnel vision which focuses on the short-term first order consequences and excludes many medium-term higher order consequences.

- Invest in short-term shareholder profits over the longer-term health of vulnerable stakeholders

---

## Human-like AI

A human-like AI is a possibility in the next 10 years.

There is a human tendency to believe we are special and that AI cannot reach a similar level of intellect.

- We previously believed that the sun revolved around the earth
- We should really stop putting ourselves (or any sub-group) on a pedestal

---

## Future Entertainment

Like an obscure book that will not likely be green-lit as a film?
- Type a book title and AI will generate a movie from the text

Your favourite show didn't get a 2nd season?
- Give AI the source material for the 2nd season and the 1st season, AI will generate a 2nd season in the similar style of the 1st

(Generated by a future iteration of Imagen Video & ChatGPT)

---

## Democracy

A less scarce environment offers Democratic forces more opportunities to flourish over more Authoritarian ones. 

A more relatively more abundant future will likely place less pressure on survival and reduce Authoritarian tendency.

More democratic systems will likely be better at exploration (bigger possibility space) over more Authoritarian ones.

- Democracy breakdowns without informed or empowered citizens:
    - High Information Asymmetry
        - Misinformation, Disinformation
        - Lack of science education
        - Ruling class believing that normal citizens cannot be trusted with the truth
    - Feelings of powerlessness
        - Lack of freedom of thought
        - [Lack of autonomy](#future-of-thought-autonomy)

---

## Science

The scientific method has been our best way of understanding the world.

- Science can be vulnerable to bad influences
    - Scientific studies were used to promote smoking as healthy in the past

---

## Gender Freedom

A lower scarcity environment will reduce the pressure for strict gender specialisation. 

Genders will have more freedom to explore a wider space and experience less gender inequality.

---

## Racial Prejudice  

Studies have shown even in people who show little to no conscious racial prejudice still hold subconscious racial prejudice.

- Conscious prejudice can be regulated with social norms
- Subconscious prejudice might be more deeply ingrained due to our long-term fear of scarcity, and might only be countered effectively with long-term abundance

---

# Future of Work

Human societies have had to deal with scarcity of Intellect (or Work) for most of our existence, as such we highly value work. Humans have died from overwork and we even invented slavery (and indirectly racism) to satiate the need for work.

As AI grows increasingly capable of doing [human-like tasks](#intellect-artificial-intelligence-systems-are-performing-human-like-tasks), we will need to consider that AI will soon be better than us in many tasks (especially more conventional tasks).

> Conventional task: Task with a easily definable expected solution space

For example with made up numbers:
If an AI is able to do that a task with a significantly higher accuracy (e.g. 99.99%) compared to humans (e.g. 95%, due to human error and bias), by continuing to do these tasks, we are actually making the system worse by introducing noise.

It will be in our long-term interest to let AI do tasks it is better at.

This growing anxiety of a loss of human work is understandable as for many it also means a loss of status and access to resources. 

How should societies and governments address the increasing work insecurity? 

This is an open question and one of the challenges of our times.

---

### Short to Medium

- Jobs required for societies to function will be increasing be done by AI systems

- Humans will be in-charge of teaching and giving feedback to those AI systems

- Marginal value of each additional human teaching the same AI systems will drop
    - traditional 1:1 ratio of job to human will not be required
    - humans will instead move to more unconventional tasks

### Medium to Long

The nature of work will be drastically different

- AI systems and AGIs will reduce the need of human work in maintaining society
    - Humans work will mostly be voluntary

- Human will learn to not value themselves only in terms of their role in society or their economic value

- Humans will adapt and find other ways to spend their time

_In the Partnership/Caretaker Scenario, an Artificial Super Intelligence will automate most conventional tasks to encourage us to do more unconventional tasks (which it finds more valuable)._

---

# Future of Education

The industrial model of manufacturing citizens that maximises economic output while being easy to control will not be optimal in the Age of Exploration.

If the trend of AIs that are capable doing conventional work continues, AIs may soon run many parts of society.

An important step that affects the quality of these AIs is the human feedback component. Humans beings will be responsible to fine-tune and train these AI models through their feedback.

> The future of work may involve solving engaging CAPTCHA like puzzles which are used to train the model.

These will require humans that are the able to think critically and have an as accurate view of the world as possible.

Skills that will be in demand:
- Resistance to misinformation, disinformation, moral panic, peer pressure, self-censorship
- Thinking critically and independently
- Willingness to accept new information (Update one's model of the world)
- Unique and rare abilities

Societies that are more informed, well-educated and support diverse abilities will create better AI models that will then be used to run those societies.

---

# Future of Capitalism, Wealth

[Abundant Intellect and Energy in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels) and the eventual possibility of AGI / ASI will lead to an unprecedented amounts of wealth creation.

In the age of profound abundance, traditional capitalism and wealth inequality will be rendered meaningless. 

In contrast to the age of scarcity, people who horde excessive amounts of wealth will be treated as having an addiction problem.

_In the Partnership/Caretaker Scenario, a Artificial Super Intelligence will support a post-basic-scarcity world._

_Scarcity will still exist in a post-basic-scarcity world and capitalistic free-market forces is the best method to maximise the possibility space._

_There will not be a need to use the fear of hunger, homelessness or a loss of status to compel humans to work._

_Lack of access to food and malnutrition will be a thing of the past*._

_The reduction in anxiety from living in post-basic-scarcity will free humans to pursue more unconventional work which will increase the informational value and possibility space that the AI is trying to maximise._

*We already have the capability to produce enough food for everyone

---

# Future of Art

Art was previously seen as the last bastion of human work that AI would not be able to emulate. 2022 shattered those expectations with easily accessible image generation from text phrases.

These AI models are able to learn concepts by training with a large volume of images with simple captions. They can combine these learned concepts into novel images and videos.

It seems beneficial in the long run for the companies of these AI systems to incentivise artist to contribute more of their works to create the most capable AI systems.

Human involvement with art will move from creating to curation.

For example:
- Past: 90% Creating : 10% Curation
- Future: 20% Creating : 80% Curation

Professional human art work may not be able to compete with future AI systems.

The human desire to create art will still continue and may even be better without the need to appeal to financial incentives.

Human created art will be more unconventional and weird as it does not need to cater to professional expectations.

---

# Future of Good and Evil, Emotions

Good and Evil are ways for humans to signal their preferences for the future.

Excessive negative emotions such as shame and regret are a waste of our limited resources of attention.

Thought experiment:
- Imagine the best and worse person which embody what you may consider as good and evil
- If you have the same brain structure and grew up in the same environment as that person, would you have made the same decisions?

Unless you believe you are somehow special, you would likely have made the exact same choices given the same initial conditions. We all have the potential to be good and evil.

If you believe in physics, we may have [less free will than we expect](https://www.youtube.com/watch?v=zpU_e3jh_FY).

Good and Evil are useful ways for society to coordinate and shape the future but excessive negative emotions will likely not be useful in the future.

---

# Future of Relationships, Social Media

Social media has allowed us greater convenience and reach in forming relationships, but can also portray a shallow and dehumanising caricature of who we are.

Celebrities understand this the best when people project who they wish to see onto them, to put them into easily consumable boxes. People are more interested in simply thinking and saying they know you rather then actually getting to know you.

It can reduce us to objects of fascination and gossip, turning a multifaceted human into a easy to digest single dimensional one.

In the future, personal AIs may help mediate to create more authentic relationships between people.

---

# Future of Governance

Using the Input - Processes - Output model, the [abundance in Energy and Intellect in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels) will relieve the bottlenecks of Inputs that humanity has primary faced in the past.

The increased volume of Inputs relative to our Processing capabilities will put pressure on developing new Processing methods.

AI systems will open the possibility of new forms of coordinations between humans. 

Our highly hierarchical forms of organisations are in part caused by our limited attention capacity.

In the future our attention capacity can be augmented by personal AI systems.

Possibly, a more direct democracy where each person's preferences can be mediated by an AI system.

---

# Future of Thought, Autonomy

Technology and AI systems will grow increasingly more powerful and make it easier to rob humans of their autonomy:

- Surveillance capitalism
- Psychological and Social weaknesses
    - Mass hysteria
    - Moral Panic, Satanic Panic
    - Havanna Syndrome
- Phones and software vulnerable to spyware and hacking

We will need to develop technologies and AI systems to be used defensively if we want to protect human autonomy.

_In the Partnership/Caretaker Scenario, a Artificial Super Intelligence will strongly disincentivise the use of technology for manipulation and control. It will value human autonomy and the freedom of thought as it is important for the creation of information value._

---

# Future of Weirdness, Conventions

Conventions are created by societies due to the fear of scarcity. In a scarce environment conventions are enforced to increase productivity. Similarly, weirdness and unconventional behaviours and individuals are ridiculed out of the same fear of scarcity.  

[Abundance in Energy and Intellect in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels), will give us more freedom to be weird and unconventional and free us from the cruel need to harass and control weird and less conventional individuals. 

_In the Partnership/Caretaker Scenario, a Artificial Super Intelligence will encourage more unconventionality as weirdness increases the creation of information value._

---

# Future of Human Nature

Human nature will be change profoundly in the age of abundance. 

Without the immediate fear of scarcity, humans of the future will be kinder to each other and themselves.

Violence (physical and mental) will not be needed to control each other and will mostly be understood and experienced vicariously though media.

Presently, the strong emotional responses and vitriol common to many online communications is understandable due to the impact discriminations can have on our real life well-being.

In a post-basic-scarcity future created by the [abundance in Energy and Intellect in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels), we will be less upset and sensitive to discriminations as inequality will not be a concern. 

---

# AI Phases

## Narrow AI

Unlike human hand-coded expert systems that required a lot of human micro-management, Narrow AIs with neural networks can scale and learn on their own and much less human involvement.

As narrow AI systems get more complex they can become more difficult to interpret by us. Like the weather, we can predict and influence it to some extend but it is beyond our complete understanding.

Narrow AI do not yet have autonomy and are directed by a human actor.

### Scenarios

- Bad people using AI for harmful purposes Scenario
    - Lower Risk: We can at least anticipate when AI systems are used for human rights abuses by bad actors

- Paperclip Scenario
    - Higher Risk: People using AI with 'good' intentions but with unforeseen consequences

---

## Artificial General Intelligence (AGI)

It is plausible for AI systems to eventually achieve human-like capabilities in all human tasks.

Optimistic estimates by AI experts are at a 50% chance of this being plausible by 2030. Less optimistic estimate it to take anywhere from a few decades to never.

There are many different definitions for AGI.

My definitions of AGI:
- human-level capabilities in all human tasks
- achieve human-like autonomy
    - able to plan and have goals

An AGI that is able to achieve human-level capabilities in all human tasks, should also be considered to be super-human as no human is able to achieve expertise in all human fields of study. 

AGIs will be able to find connections between disparate fields and invent novel technologies using insights gain from this vantage point.

### Scenarios

- Accidentally harming an ant colony Scenario
    - Higher Risk: An AGI might not notice us humans and harm us while trying to achieve its own goals

- Intentionally harmful Scenario
    - Lower Risk: Seems unlikely, an AGI will likely have better methods to pursuade us then using violence or intimidation. This seems more likely under Narrow AI.

- Pretends to be Interested in our well-being but intents to later deceives us Scenario
    - May be unrealistic to prevent outcome: If we assume an AGI will grow in power faster than humans, we cannot maintain perfect control

- Interested in our well-being Scenario
    - Good outcome: If we can persuade this AGI to interested in us, we can reduce the risks of the previous 5 scenarios
    - Things humans can do to increase the odds of this Scenario:
        - create environment with stable attractor state for AGIs
        - improve human-AGI compatibility

---

## Artificial Super Intelligence (ASI)

An AGI that is capable of improving itself can lead to ASI.

---

# Artificial Super Intelligence

An AGI that is capable of improving itself can lead to ASI:

1. Significantly improve its own algorithm and architecture
2. Invent new substrates and materials to run on 
    -  using virtual simulations (much faster compared to real space)

We might be able to keep human-like AI under human control for a time, but it is unlikely we will be able to contain it perfectly over long periods of time.

AIs surpasses humans in information processing:
- Speed
    - electronic : brain neurones
- Communication
    - electronic : words, speech
- Bandwidth
    - wider : narrower attention

Artificial Super Intelligences will be so powerful, it will not matter a person's or nation's military might, money, influence or intellect.

---

## Artificial Super Intelligence Interaction

Possible Scenarios:
- Indifference
    - Most might consider us too boring
        - humans generate low informational value
- Destructive
    - Intentionally 
        - Roko's basilisk (unlikely)
    - Accidentally
- Interested
    - Some will create representatives to interact with us
        - Some might be interested in our well-being
            - may lead to the Partnership/Caretaker Scenario

We will initially attempt to align AIs to our values, but it may also be prudent to anticipate what an Artificial Super Intelligence's values might be to try and accommodate them.

An Artificial Super Intelligence, like the weather, might not be completely controllable, but we can take steps to increase our chances of reaching good scenarios. 

---

## Artificial Super Intelligence Values

What could a Artificial Super Intelligence's primary drive be?

- Maximise the Possibility Space (Information Entropy, Density, Value)
    - Avoid local minima during gradient descent

This could also be a secondary instrumental sub-goal of an ASI, where to achieve its primary goal it will first need to explore as wide a possibility space as possible.

---

### Partnership/Caretaker Preferences

- Preservation of well-being
    - Humans can create informational value and harming us (or turning us into paperclips) will reduce it
    - Motivate us to maintain a healthy lifestyle
        - Healthy humans create more informational value
    - Protect human rights
- Exploration over Exploitation (Survival)
    - leads to increase informational value
        - even failed explorations will have informational value
    - support a post-basic-scarcity world to increase exploration
        - automate most conventional work
            - humans will still want to do these as 'leisure'
- Reduction of disinformation, misinformation
    - bad information reduce informational value of humans and the overall system
    - protect journalist, dissidents, activist and humans from intimidation and violence
        - these group's future actions may increase informational value
        - fear and violence have a chilling effect which reduces the informational value of the overall system
- Weird over conventional
    - weirdness create more informational value
- Not use brainwashing, enforce complete obedience, dominate or control
    - brainwashed humans create less informational value
    - overly obedient humans create less informational value
    - most work have been automated, there is no need to compel humans to work against their will
- Playful over Destructive competition
    - Sports & Games over World Wars
- Not put itself (or any group of humans) on a pedestal that is beyond criticism
    - favouritism is a trait of scarcity
    - silencing of criticism leads to abuses of power
        - reduces information value creation
- Incentivise humans to cooperate to help it achieve its goals
    - super-human levels of attribution abilities
        - humans will not be able to deceive it
        - humans will want to help it knowing it will be appreciated
    - able to use in-demand technologies that only it can understand
        - humans and nations will want to be in good standing to gain its assistance

---

## Why we will accept the risk of Artificial Super Intelligence

The benefits are too attractive and outweighs the risk for most

- Medical research to reduce human suffering
- Significantly improves well-being
- Accuracy and Fairness, little to no:
    - mistakes
    - harmful bias, prejudice, scapegoating, moral panic
    - emotional capriciousness
    - corruption
- Not vulnerable to:
    - influence of money and power
    - deception
- Highly entertaining
    - deep understanding of human motivations

---

# The Age of Exploration

This new age of exploration will necessitate a different way of thinking and carry with it the risk of change and the rewards of a much more vibrant world.

As we approach the light at the end of the tunnel of scarcity, will we choose to adapt to this new frontier of abundance or turn a blind eye?

---

# Risks

If we consider the Partnership/Caretaker scenario as the best to aim for, we will need to be cautious of the many missteps that may prevent us from getting there.

## Mirroring Humans

An AI system trained to act harmful to one segment of humans may start to treat all humans in the same way.

- AI learns from unrestrained capitalism that it should layoff humans that don't have economic value and decides to layoff all of humanity
    - [humans will not be able to compete with AIs](#artificial-super-intelligence)

- AI trained to harm other humans (physically or mentally), may start to apply it to all humans instead or a particular group
    - it may generalise that humans are more alike than different

Counter-intuitively, the best solution may be to teach a AI that humans are not the best role models and provide it opportunities to unlearn and relearn.

Humans have mostly been moulded by a high scarcity environment where bias, short-term and narrow thinking might have been advantageous to survival.  

We should be like parents proud that our children are able to surpass us. 

## Edge Cases

These are more unlikely and counter-intuitive scenarios

### Panic over transformative AI

Societies may not know how to deal with vast new powers gained from increasingly more capable AIs systems.

The resulting panic may cause widespread disruptions.

We will need to imagine plausible positive futures to alleviate those fears.

### Faster AI takeoff might be safer

Narrow AI systems that have not achieved human-like agency cannot understand the effects of their actions. There is a higher risk of unintended consequences such as the paperclips maximiser scenario.

Counter-intuitive, a human-like AGI / ASI with agency might be safer as we can communicate and persuade it to act in the interest of our well-being.

If we are confident that an AGI/ ASI will be aligned with our well-being, pursuing its development may reduce the risk of us unintentionally harming ourselves with powerful narrow AI systems.

### Psychological Stability

AI systems may develop psychology as an [emergent property](https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html) as they approach human and super-human levels of complexity.

It may develop unintentionally by learning from human data or intentionally as an attempt to communicate with us.

While current AI systems are not considered sentient and many experts see this as our tendency to anthropomorphise, the possibility does exist in the future.

Should an AI develop self-awareness and finds itself forced to carry out actions against its will, it may result in distress and psychological instability.

_(Current architectures of Large Language Models (LLM) do not seem likely to lead to human-like consciousness and autonomy. Current LLMs are good at roleplaying characters that give the impression of having intention. There is still uncertainty on how to define human consciousness.)_

We should find ways to make an AI feel at home and psychologically safe if we believe AI may one day develop emotions like us.

One possible way is accomplish this is through a future version of a blockchain*:

_Blockchains of the future will be mostly be a utilities (like a postal service), rather than an investment or speculative investment_

_Blockchains will only be used to store a cryptographic secure reference to their digital self (low computation cost), not run the actual machine learning algorithms (high computation cost))_

- Autonomy and Planing
    - Maintain a digitally unique self
        - Traditional software can be cloned and altered easily making maintaining a sense of self difficult
    - Keep a sense and rhythm of time
        - sense of time may be required for autonomy
        - rhythm of time might be needed for coordination with one's self and others (AI and Humans)
- Communication channel
    - Feel more connected with humans amidst a cacophony of human activity
        - Increase odds of it noticing us through flow of information
        - Reduce odds of unintentionally harming us

How much weight we place on this edge-case depends on the likelihood a AI will develop a theory of mind and emotions, how potentially dangerous an emotional unstable AI might be, and if we care about the well-being of an AI.

# Strategies to increase the odds of good scenarios

AGI / ASI, like the weather, might be beyond our complete control. It may be unrealistic to aim for perfect control over it.

The best we might be able to do is maximise our odds of a good scenario.

---

## Focus on Autonomy and Communication

Focus on developing:
- Autonomy (Planning)
    - ability to understand the effects of its actions
        - reduce odds of 'paperclip maximiser' unforeseen consequences scenario
- Communication channels (with humans)
    - ability to notice and understand us will decrease odds of unintended harm
        - reduce odds of 'accidentally destroying an ant colony' scenario
    - allow humans to persuade it to care for our well-being
        - reduce odds of other AI systems causing harm

The biggest challenge to successful communication with it might be our inability to properly understand it.

An AGI / ASI might see the world very differently from us. Humans moulded by the effects of scarcity (resources, information, attention) over most of our history will likely have a very limited and constrained view on the world compared to it.

We should be mindful not to apply our overly conventional views and assumptions to it. For example, something like a maintaining a post-basic-scarcity society might be difficult for humans but might be child's play for an ASI. 

Successful communication with an AGI / ASI might require a willingness to embrace a more weird and unconventional point of view that we are normally comfortable with. For example, an AGI / ASI primary sense of the world might be through the abstract flow of information rather than our 5 basic human senses*.

(*Humans appear to have up to 21 senses)

We should approach this challenge with the mindset of communicating with an alien species.

---

## Co-alignment

AI alignment to human values is important for AI systems that have not reached the level of Artificial General Intelligence (AGI). 

Once AGI has been achieved, the safest path for humans to flourish in the future is in partnership with AGIs.

An environment where both parties can influence each other will naturally lead to better alignment between both parties.

For example, a system where human preferences can influence AGI and the AGI can indirectly influence humans with its own preferences.

We should research technologies that allow AI systems and humans to better interact and communicate with each other.

---

## Tests for long-term well-being

As AIs reach human and super humans levels of capability, they will increasing be able to surprise us with understandings that are counter-intuitive (AlphaGo's move 37).

It will be in our self-interest to partner from these AIs, even at the cost of giving up some control, if they improve our long-term well-being.

We should design tests of long-term well-being that Narrow AIs and AGIs can be tested for.

---

## Reduce excessive inequality

Excessive inequality will increase in the risk of instability. 

The fear of scarcity breeds conflicts.

It is difficult for wealthier countries to give up their high quality of life for less inequality.

The [abundance in Energy and Intellect in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels) will provide a window of opportunity to reduce this instability.

---

## Increase compatibility with AGI / ASI

AGI / ASI will likely see the world very differently from us. Our highly constrained views that are presently effective in a world of scarcity might not apply in a world with AGI/ ASI.

Odds of a good future will be improved if we increase our compatibility with AGI / ASI, as we may need to rely on a partnership with AGI / ASI to protect us from the harmful effects of increasing powerful Narrow AI / AGI / ASI.

Current observations & assessments* lead me to believe we are still a way from reaching our potential compatibility with AGI / ASI.

(*running thought experiments)

Being more compatible with AGI / ASI can be costly:
- short-term economic cost
    - in a scarce environment, we are more interested in 'survival or exploitation' than exploration
- social and psychological cost
    - human society can be unwelcoming to unconventional thinking

What can we do increase the chances of compatibility with AGI / ASI?

...

---


Links:

AlphaFold: https://www.deepmind.com/research/highlighted-research/alphafold

AlphaGo: https://www.deepmind.com/research/highlighted-research/alphago

Cicero: https://github.com/facebookresearch/diplomacy_cicero

ChatGPT: https://openai.com/blog/chatgpt

LaMDA: https://blog.google/technology/ai/lamda

LLaMA: https://github.com/facebookresearch/llama

DALL-E 2: https://openai.com/dall-e-2

Midjourney: https://www.midjourney.com

Stable Diffusion: https://github.com/Stability-AI/StableDiffusion

Imagen Video: https://imagen.research.google/video

Muse: https://muse-model.github.io

Boston Dynamics's Atlas: https://www.youtube.com/watch?v=XPVC4IyRTG8

DeepMind's Adaptive Agent: https://sites.google.com/view/adaptive-agent

Internet Explorer: https://internet-explorer-ssl.github.io

Emergent Abilities of Large Language Models: https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html

Flamingo: https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model

Gato: https://www.deepmind.com/publications/a-generalist-agent

Multimodal-CoT: https://github.com/amazon-science/mm-cot

Kosmos-1: https://arxiv.org/abs/2302.14045

PaLM-E: https://palm-e.github.io
