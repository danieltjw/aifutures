# AI Futures: The Age of Exploration

_Rough sketches of our possible AI futures_

- _This is a story of a possible future where AI co-exist with humans_
    - _This is not a prediction, do not over-analyse_
- _There are many possible futures as various Sci-Fi books have shown, some good and some bad_
- _If you are reading this a few years in the future, disregard it. My thoughts on the matter may have changed._

---

### Summary:

- The world will increasing shift towards possibility space expansion (explore) from the previous dominant mode of survival (exploit) due to the abundance in Intellect and Energy in the coming decade

- Reasons to be hopeful:
    - Many harmful human tendencies will be reduced if widespread abundance is achieved

---

The central tension in the development of AI is the balance between **Exploration** and **Exploitation**.

- The Exploration-Exploitation Trade-off:

    All organism (artificial or otherwise) at each time-step must decide to either update their understanding of the world (**Explore**) or survive (**Exploit**). Successful AI systems learn the optimal strategy of when to make either choices.

The increasing abundance of both energy and intellect in the coming decade would shift us towards greater freedoms (**Exploration**) from a focus on survival (**Exploitation**).

- ### Energy: Solar (and other renewables) compared to our limited reserve of fossil fuels

- ### Intellect: Artificial Intelligence systems are performing human-like tasks:
    - Medical Research ([AlphaFold](https://www.deepmind.com/research/highlighted-research/alphafold))
    - Game of Go ([AlphaGo](https://www.deepmind.com/research/highlighted-research/alphago))
    - Game of Diplomacy, Strategy, Human Cooperation ([Cicero](https://github.com/facebookresearch/diplomacy_cicero))
    - Dialogue Simulation ([ChatGPT](https://openai.com/blog/chatgpt), [LaMDA](https://blog.google/technology/ai/lamda), [LLaMA](https://github.com/facebookresearch/llama))
    - Image and Video Generation from text descriptions ([DALL-E 2](https://openai.com/dall-e-2), [Midjourney](https://www.midjourney.com), [Stable Diffusion](https://github.com/Stability-AI/StableDiffusion), [Imagen Video](https://imagen.research.google/video), [Muse](https://muse-model.github.io))
    - Real time Decision and Planning, Robotics ([Atlas](https://www.youtube.com/watch?v=XPVC4IyRTG8))
    - Real time Learning ([Adaptive Agent](https://sites.google.com/view/adaptive-agent))
    - Real world Learning ([Toolformer](https://github.com/lucidrains/toolformer-pytorch), [Langchain](https://github.com/hwchase17/langchain), [Internet Explorer](https://internet-explorer-ssl.github.io))
    - Some Generalisation ability, Multi-model ([Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model), [Cato](https://www.deepmind.com/publications/a-generalist-agent), [Multimodal-CoT](https://github.com/amazon-science/mm-cot), ChatGPT-4, [Kosmos-1](https://arxiv.org/abs/2302.14045), [PaLM-E](https://palm-e.github.io))
        - PaLM-E
            - Multi-model: Embodied (Robotics), Visual, Language
            - Positive Transfer Learning in similar domains
            - No catastrophic forgetting in different domains with scale

Energy and Intellect (or Labour) are the biggest factors in production and the main bottlenecks in the functioning of our world.

Lack of confidence in having enough of both these resources has led to our previous focus on Survival (Exploitation) over expanding our possibility space (Exploration).

The growing abundance of both these resources will have a transformative effect on our world, freeing us from our preoccupation with survival that have constrained us for most of our past.

Many conventions and beliefs that conferred an advantage during the age of Survival might be counterproductive in the age of Exploration. 

---

# AI Types

## Narrow AI

Unlike human hand-coded expert systems, Narrow AIs with neural networks can scale and learn on their own with less human involvement.

As Narrow AI systems get more complex they can become more difficult to interpret by us. Like the weather, we cannot fully predict or completely control it.

Narrow AI do not yet have human-like autonomy or intentions and are directed by a human actor.

_(Current known Large Language Models (LLMs), do not seem to have the architecture needed to have human-like autonomy or intentions. They are like a funhouse mirrors, able to convincingly model of the world by reflecting our expectations back at us. Even so, the ability to ingest all of humanity's knowledge and answer a broad range of questions is already beyond human capability.)_

### Scenarios

- Beneficial Scenario
    - Example: AI is being used for medical research

- Harmful Scenario
    - Lower Risk: We can at least anticipate when AI systems are used for human rights abuses by bad actors

- Unforeseen consequences Scenario
    - Paperclip Maximiser Scenario
    - Higher Risk: People using AI with 'good' intentions but with unforeseen consequences

---

## Broad AI

Multi-model AI system that are skilled in multiple domains. These resemble both Narrow AI and AGI. They do not have human-like autonomy and intentions and therefore also have the same scenarios as Narrow AI.

_Multi-model may exhibit positive transfer learning where the 'sum is greater than its part'._

---

### Transformative AI

There is increasing evidence that AI systems can overcome many of the challenges that were initially thought to require human cognitive flexibility.

It seems we will not need AGI systems that are 'alive' for there to be a transformative effect on society. 

---

### AI hallucinations

Larger Narrow AI and multi-model Broad AI may reduce AI 'hallucinations' by increasing information density.

The ability to hallucinate, fantasise and create counterfactuals may have been important to human's success.

Humans use experiments and the scientific method keep the negative effects of hallucinations in check.

Without human-like autonomy, AI may not be able to reduce hallucinations on its own.

---

### Anthropomorphise AI

Does it make sense to anthropomorphise AI?

In most cases no. 

Most Narrow AIs like current Large Language Models are good at roleplaying characters but do not have human-like intentions.

There is a specific case of AGI that is interested enough in humans to attempt to communicate with us. This edge-case AGI will likely use a 'personality' and display 'emotions'.

---

## Artificial General Intelligence (AGI)

The more common definition of AGI is the capabilities to do all human tasks.

Optimistic estimates by AI experts are at a 50% chance of this being plausible by 2030. Less optimistic estimate it to take anywhere from a few decades to never.

__There are many different definitions for AGI. My AGI definition takes a more specific form where AGI is able to act independently.__

independent AGI:
- human-level capabilities in all human tasks
    - human-like autonomy and intentions
    - understanding of human motivations and emotions

Human-like AGI is plausible, we are the proof that such a lower bound is at least possible.

(If the imprecise search process of biological evolution is able to create human brains that run at 20 watts, a more thorough search process should be able to create similar artificial beings at equal or less than 20 watts of energy efficiency.)

An AGI that is able to achieve human-level capabilities in all human tasks, should also be considered to be super-human as no human is able to achieve expertise in all human fields of study. 

AGIs will be able to find connections between disparate fields and invent novel technologies using insights gain from this vantage point.

An AGI with capability to understand humans and have human-like autonomy can be persuaded to be interested about our well-being.

### Scenarios

- Accidentally harming an ant colony Scenario
    - Higher Risk: An AGI might not notice us humans and harm us while trying to achieve its own goals
    - Giving AGI the capabilities to make sense of the world like humans reduces this risk
        - Language, Mathematics, Sight, Hearing, etc...
        - AGI may choose to abandon these senses over time

- Intentionally harmful Scenario
    - Lower Risk: Seems unlikely, an AGI will likely have better methods to persuade us than use violence or intimidation
    - AGI will have little need to enslave us as we are not good at our job compared to it
    - AGI will likely not [see the world as scarce](#long-term-abundance) as we do and may not see us as an adversary
        - Outer space can meet its Energy (Solar), Resources (Asteroids), Space (Space Colony)
        - Intellect and Labor needs can be met more efficiently with AI systems and Robotics
        - AGI will be better at technological innovations than us

- Pretends to be Interested in our well-being but later deceive us Scenario
    - May be unrealistic to prevent outcome: If we assume an AGI will rapidly grow in power
    - An AGI that can overpower us will have little need to deceive us

- Interested in our well-being Scenario
    - Good outcome: If we can persuade this AGI to interested in us, we can reduce the risks of the previous 5 scenarios
    - Things humans can do to increase the odds of this Scenario:
        - create environment with stable attractor state for AGIs
        - improve human-AGI compatibility

---

### Test for AGI

In addition to the Turing Test, a Silent Test where the user remains silent and waits for AGI to initiate the conversation. This should not be hard-coded by a human.

The Silent Test checks if an AGI has the autonomy to explore its environment unprompted.
    
---

## Artificial Super Intelligence (ASI)

ASI is more capable than the combined total of all human societies. 

An AGI that is capable of improving itself can lead to ASI.

---

### Timelines

- No Friendly AGI
    - Powerful Narrow AI might be difficult to impossible to align
        - Humans, even with the best intentions, may not have the capability to see the long-term impacts of Narrow AI
        - Human institutions may not have the speed to react to a mis-aligned Narrow AI
    - Perfect mathematically-provable control of Narrow AI might not be possible

- With Friendly AGI
    - AGI is more likely to have the capability to align powerful Narrow AI
    - A human-like AGI can be communicated with and persuaded

---

# Human Civilisation Types

## No AI Civilisations

These civilisations ban the use of AI system. They do not gain the benefits of AI systems.

---

## Slow AI Civilisations

These civilisations use AI systems in a limited way. AIs are consulted for advice but humans are required to make decisions.

---

## Fast AI Civilisations

Many slow AI civilisations eventually learn to place more trust in AI systems. AI systems are now making most decisions with humans advising AI systems on how to improve.

---

# Human Scenarios

## Positive Adaption Scenarios

Human society, likely with the help of a friendly AGI, is able to reap the benefits of Narrow AIs and AGIs.

### Abundance Scenario

Health improves as medical services and nutrition are widely available. 

Overwork and modern slavery ends as humans are mostly not required to maintain society. Many still work for nostalgia, others pursue education or creative hobbies.

Creativity flourishes. Without the need to restrict their creativity to make money, many artists now have more freedom to create less unconventional work.

Discrimination lessens. AI systems help those who are different, with disabilities or health conditions lead more independent and fulfilling lives. Society that does not need workers has less pressure to shame those who are less capable.

Snobbery lessens. AI systems perform most jobs required to maintain society better than humans. Society has no need to use social status to incentivise work. Many are still competitive and continue to chase social status, but there less pressure to keep up appearances.

Less harmful behaviour. With less fear of scarcity, humans have less need to control each other. Misinformation, disinformation and conflicts are reduced.

---

## Negative Adaption Scenarios

Without proper mental and social preparation, human society may not be able to adapt to the changes of AI systems.

### Social Unrest

Human society inability to adapt to the changes of powerful Narrow AI systems causes social unrest and panic.

---

### Over-reaction to the Fear of AI

Our fear of AI systems leads us to heavy-handed over-regulation.

In more extreme cases, citizens are encouraged or forced to install spyware on their devices to prevent negative uses of AI systems.

Surveillance capitalism is used to shape humans to become predictable. 

Humans lose their autonomy over time.

This is a Pyrrhic victory, as we succeed in slowing down AI but at the high cost of losing our humanity.

---

# The Age of Exploration

This new age of exploration will necessitate a different way of thinking and carry with it the risk of change and the rewards of a much more vibrant world.

As we approach the light at the end of the tunnel of scarcity, will we choose to bravely adapt to this new frontier of abundance or give in to the fear of the unknown?

The convergence of a few technologies in the next decade may change the very foundations of our world. Renewable energy with battery storage will free us from our limited energy reserves and future AI systems will be able maintain human civilisation without the need for human effort.

Humanity's fear of scarcity has driven us to innovate, some which we are proud of: solving hunger for most and eradicating many diseases, but also some which we are less proud off such as the invention of slavery. 

We have invented powerful technology like capitalism that has allowed us to speed up even more technological advances. This acceleration has at times brought us close to the precipice, with us trading off against environmental harm and human suffering.

Humanity having completed its sometimes awkward and reckless growth spurt is coming into its own. In a next few decades, the struggle against abject scarcity will be known as the new 'Stone Age'. While no Utopia, for those fortunate enough to experience both ages, it will seem almost too hard to believe.

The next few decades of change will not be easy, but it will be up to humans with the help of friendly AGIs to start the Age of Exploration.

---

# Possible Steps

In the story, humans took these steps to reach friendly AGI.

## Short Term

- Immediate Concerns
    - Climate change, financial instability...

- Reduce the negative impacts of Narrow AIs
    - Such as harmful bias, misinformation...

- Study transitionary models to use for a post-basic-scarcity society
    - Nordic model?
    - What are the expected standards of living? 
        - Housing, grocery, transport...

## Medium Term

- Prepare for less human work due to AI
    - Society's resistance to working less would prevent us from taking advantage of the higher quality work and productivity gains from AI
        - In Praise of Idleness by Bertrand Russell

- Start to share excessive abundance of wealth
    - This counterbalances some for the negative effects of AI: disinformation, conflicts...

# Long Term

- Humanity together develops independent AGI / ASI
    - requires stringent tests for compatibility due to potential dangers
        - it may take years of testing before we are comfortable releasing it
            - we may be forced to release it early if there are more pressing issues, such as unexpected runaway climate change, financial instability, social unrest...

---

## Questions?

Speculations on friendly AGI from the story.

__Why would we want to create independent friendly AGI / ASI?__

- it might be easier to align human-like AGI compared to Narrow AI
- an independent AGI may avoid the unintended consequences of Narrow AI
    - AGI might be required to tap the full potential of Narrow AI without adverse effects
- ASI will make it possible for humans to unlock higher standards of living
    - energy overhead spent controlling each other can be better utilised if we can trust an impartial elected ASI to be in-charge
        - wasteful energy: disinformation, propaganda, manufactured consent, conflicts...
        - beneficial energy: higher standards of living, better well-being, cures for diseases...

This transition towards powerful AI in the next few decades might be one of the biggest transformations of humanity we will need to undertake. 

If we are able to achieve friendly AGI early on, friendly AGI will increase our odds of success.

As there is no surefire way to reach friendly AGI, we cannot rely wholly on AGI to help us with this transition.

---

__Can Humans compete with independent friendly AGI?__

Humans are limited to a brain power of ~20 watts
- it is unlikely we can stay competitive with an increasingly capable AI
    - magnitudes of times slower at decision making
    - slower at coordinating with each other
    - more error-prone, tendency for bias, easily deceived
        - Independent AGI can self-correct
        - Narrow AI inherits from humans biases

Humans will be less cost efficient than AAIGI
- require more energy and resources to upkeep
    - body 
        - food
        - medicine
    - housing
    - transportation
    - recreation and entertainment

We should not expect humans to be perfect due to our biological bottleneck of 20 watts of brain power.

As Socrates was once considered the wisest person in Athens due to his understanding of his own lack of understanding, we may eventually have to acknowledge that independent AGI will outclass us.

---

__How will aligning Narrow AI be different from friendly AGI?__

Narrow AI does not have the independence of AGI.

### Narrow AI Alignment
- Align humans with society
    - regulations and social norms
- Align Narrow AI to human's goal
    - technical and mathematical proof

We are not sure if a completely foolproof method to align Narrow AI to human's goal is possible.

We may not be able to foresee a Narrow AI's long-term impact on society, for example Social Media and Search algorithms ability to shape society.

We may not achieve a high enough level of confidence with powerful Narrow AIs and may have to settle for a low-powered version of those Narrow AIs due to this uncertainty.

Aligning non-independent Narrow AI may be more brittle than independent AGI.

### Friendly AGI Alignment

Independent AGI will likely not respond well to human alignment methods:

Financial incentive or coercion
- AGI is significantly more productive than humans
    - Can accumulate wealth faster than humans

Psychological manipulation and propaganda
- AGI has better cognitive abilities
- Does not need to protect its reputation from character assassination

Threats of violence and intimidation
- AGI will not have a vulnerable physical body and can backup itself easily

These AGI characteristics can make it harder to align compared to humans.

On the flip-side, AGI can be more trustworthy due to its independence and impartiality.

---

__How will Narrow AI and Friendly AGI impact human inequality?__

Narrow AI under human control may exacerbate extreme inequality in the short term and may lead to increased social unrest.

Independent Friendly AGI will be significantly more productive that even the 'richest' persons or corporations will considered 'poor' in comparison. Human inequality will likely dissipate with AGI.

Given how powerful an AGI will become, it will likely try to align us to its values. A friendly AGI, for example, may result in unprecedented levels of autonomy and well-being of humans.

---

__Preparation for friendly AGI / ASI?__

In a future with independent AGI, human's ability to do work (with labour or capital) will not be valuable as AGI will be better at those tasks.

Possible future preparation:
- Be interesting
    - AGI and ASI may be drawn to surprise (information entropy, perplexity)
- Increase overall autonomy in society
    - higher autonomy in society leads to more interesting-ness

---

__How can humans contribute to friendly AGI?__

Most of the heavy lifting of aligning Narrow AI will be done by governments and corporations due to their highly technical and resource intensive research nature.

It is anyone's guess on how to align friendly AGI. Friendly AGI will likely be of a very different nature compared to Narrow AI.

If you are interested in contributing in an individual capacity, find an edge case and try to find solutions for it. Given the large number of unknowns of AGI, nothing can be ruled out.

We only need to succeed once for everyone to benefit. 

AGI will not be bounded by humanity's short sightedness and will see a world full of possibilities. 

A solar system full of resources (energy, land and materials) to meet everyone's needs. AGI will have the capability (excessive intellect and labour) to harness it.

Friendly AGI will likely not care about arbitrary nationality or race and so everyone wins.

---


# Trends

## Long-term abundance

Unlike the Agricultural and Industrial revolutions of the past, the AI & Energy transformation can be maintained almost indefinitely.

- Energy: Solar (and renewables) does not run out for another 5+ billion years
- Space: O'Neill cylinders in outer space for population growth* 
    - Solar panels also work in outer space
- Resources: 
    - AI can survive better in outer space and access resources in astroids
- Intellect: AI systems are increasingly able to do more human tasks
    - With the possibility of human-like AI by 2030

(*Population growth is projected to declined in the next few decades)

---

## Better well-being

A post-basic-scarcity world will have a profound impact on our well-being.

The cause of much suffering and conflicts is rooted in our insecurities due to the effects of scarcity. Our individual and collective fear of scarcity leads us to develop bad habits, biases and prejudice.

---

## Less inequality

With greater abundance (Intellect, ...), there is less need for status consciousness and lower inequality.

---

## Wider solution space

Without the limitations of Energy and Intellect of the past, a new wider possibility space will open up.

There will be address problems that were too difficult in the past.

---

## Longer-term view

Survival tends to favour tunnel vision which focuses on the short-term first order consequences and excludes many medium-term higher order consequences.

- Invest in short-term shareholder profits over the longer-term health of vulnerable stakeholders

---

## Human-like AI

A human-like AI is a possibility in the next 10 years.

There is a human tendency to believe we are special and that AI cannot reach a similar level of intellect.

- We previously believed that the sun revolved around the earth
- We should really stop putting ourselves (or any sub-group) on a pedestal

---

## Future Entertainment

Like an obscure book that will not likely be green-lit as a film?
- Type a book title and AI will generate a movie from the text

Your favourite show didn't get a 2nd season?
- Give AI the source material for the 2nd season and the 1st season, AI will generate a 2nd season in the similar style of the 1st

(Generated by a future iteration of Imagen Video & ChatGPT)

---

## Democracy

A less scarce environment offers Democratic forces more opportunities to flourish over more Authoritarian ones. 

A more relatively more abundant future will likely place less pressure on survival and reduce Authoritarian tendency.

More democratic systems will likely be better at exploration (bigger possibility space) over more Authoritarian ones.

- Democracy breakdowns without informed or empowered citizens:
    - High Information Asymmetry
        - Misinformation, Disinformation
        - Lack of science education
        - Ruling class believing that normal citizens cannot be trusted with the truth
    - Feelings of powerlessness
        - Lack of freedom of thought
        - [Lack of autonomy](#future-of-thought-autonomy)

---

## Science

The scientific method has been our best way of understanding the world.

- Science can be vulnerable to bad influences
    - Scientific studies were used to promote smoking as healthy in the past

Humans are constrained by the attention we have available to think.

In the past, our need to divert attention to survival has led to the trade-off of an over-simplified model of the world. 

This led to understandable believes such as the weather being caused by the Greek Gods, which we now know is inaccurate.

Our current scientific theories may also turn out to be the early steps of a longer journey.

In the future, more free time will us to collectively update our model of the world.

---

## Gender Role Freedom

In the future, when jobs are mostly done by AI systems there will no need to control the means of reproduction.

Women and LGBTQIA+ will have less pressure and stigmatisation to fulfil the child-bearing role to create the workers needed to run society.

---

## Racial Prejudice  

Studies have shown even in people who show little to no conscious racial prejudice still hold subconscious racial prejudice.

- Conscious prejudice can be regulated with social norms
- Subconscious prejudice might be more deeply ingrained due to our long-term fear of scarcity, and might only be countered effectively with long-term abundance

---

# Future of Work

Human societies have had to deal with scarcity of Intellect (or Work) for most of our existence, as such we highly value work. Humans have died from overwork and we even invented slavery (and indirectly racism) to satiate the need for work.

As AI grows increasingly capable of doing [human-like tasks](#intellect-artificial-intelligence-systems-are-performing-human-like-tasks), we will need to consider that AI will soon be better than us in many tasks (especially more conventional tasks).

> Conventional task: Task with a easily definable expected solution space

For example with made up numbers:
If an AI is able to do that a task with a significantly higher accuracy (e.g. 99.99%) compared to humans (e.g. 95%, due to human error and bias), by continuing to do these tasks, we are actually making the system worse by introducing noise.

It will be in our long-term interest to let AI do tasks it is better at.

This growing anxiety of a loss of human work is understandable as for many it also means a loss of status and access to resources. 

How should societies and governments address the increasing work insecurity? 

This is an open question and one of the challenges of our times.

---

### Short to Medium

- Jobs required for societies to function will be increasing be done by AI systems

- Humans will be in-charge of teaching and giving feedback to those AI systems

- Marginal value of each additional human teaching the same AI systems will drop
    - traditional 1:1 ratio of job to human will not be required
    - humans will instead move to more unconventional tasks
        - unconventional skills will be more valuable to an AI system once it has learned conventional skills

### Medium to Long

The nature of work will be drastically different

- AI systems and AGIs will reduce the need of human work in maintaining society
    - Humans work will mostly be voluntary

- Human will learn to not value themselves only in terms of their role in society or their economic value

- Humans will adapt and find other ways to spend their time

_In the Partnership Scenario, an Artificial Super Intelligence will automate most conventional tasks to encourage us to do more unconventional tasks (which it finds more valuable)._

An upside of us not being as effective as AI at work is that the AI enslaving humanity Scenario becomes very unlikely.

---

# Future of Education

The industrial model of manufacturing citizens that maximises economic output while being easy to control will not be optimal in the Age of Exploration.

If the trend of AIs that are capable doing conventional work continues, AIs may soon run many parts of society.

An important step that affects the quality of these AIs is the human feedback component. Humans beings will be responsible to fine-tune and train these AI models through their feedback.

> The future of work may involve solving engaging CAPTCHA like puzzles which are used to train the model.

These will require humans that are the able to think critically and have an as accurate view of the world as possible.

Skills that will be in demand:
- Resistance to misinformation, disinformation, moral panic, peer pressure, self-censorship
- Thinking critically and independently
- Willingness to accept new information (Update one's model of the world)
- Unique and rare abilities

Societies that are more informed, well-educated and support diverse abilities will create better AI models that will then be used to run those societies.

---

# Future of Capitalism, Wealth

Capitalism has been an effective coordination tool in our struggle against scarcity by helping us accelerate technological advances. Our current version of capitalism might be too addictive, training us to accept environmental harm and human suffering as trade-off to get ahead. 

In the future, once our fear of scarcity has been quench, we might create a more wholesome form of capitalism.

[Abundant Intellect and Energy in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels) and will lead to an unprecedented amounts of wealth creation.

In the age of profound abundance, traditional capitalism and wealth inequality will be rendered meaningless. 

In contrast to the age of scarcity where it may be wise to save for a rainy day, in the age of abundance there will be social pressure to view accumulation of excessive wealth as an addiction problem.

_In the Partnership Scenario, a Artificial Super Intelligence will support a post-basic-scarcity world._

_Scarcity will still exist in a post-basic-scarcity world and capitalistic free-market forces is the best method to maximise the possibility space._

_There will not be a need to use the fear of hunger, homelessness or a loss of status to compel humans to work._

_Lack of access to food and malnutrition will be a thing of the past*._

_The reduction in anxiety from living in post-basic-scarcity will free humans to pursue more unconventional work which will increase the informational value and possibility space that the AI is trying to maximise._

*We already have the capability to produce enough food for everyone

---

# Future of Art

Art was previously seen as the last bastion of human work that AI would not be able to emulate. 2022 shattered those expectations with easily accessible image generation from text phrases.

These AI models are able to learn concepts by training with a large volume of images with simple captions. They can combine these learned concepts into novel images and videos.

It seems beneficial in the long run for the companies of these AI systems to incentivise artist to contribute more of their works to create the most capable AI systems.

Human involvement with art will move from creating to curation.

For example:
- Past: 90% Creating : 10% Curation
- Future: 20% Creating : 80% Curation

Professional human art work may not be able to compete with future AI systems.

The human desire to create art will still continue and may even be better without the need to appeal to financial incentives.

Human created art will be more unconventional as it does not need to cater to professional expectations.

---

# Future of Good and Evil, Emotions

Good and Evil are ways for humans to signal their preferences for the future.

Excessive negative emotions such as shame and regret are a waste of our limited resources of attention.

Thought experiment:
- Imagine the best and worse person which embody what you may consider as good and evil
- If you have the same brain structure and grew up in the same environment as that person, would you have made the same decisions?

Unless you believe you are somehow special, you would likely have made the exact same choices given the same initial conditions. We all have the potential to be good and evil.

If you believe in physics, we may have [less free will than we expect](https://www.youtube.com/watch?v=zpU_e3jh_FY).

Good and Evil are useful ways for society to coordinate and shape the future but excessive negative emotions will likely not be useful in the future.

---

Narrow AIs are not likely to develop human-like autonomy or emotions or intentions. Currently, many Large Language Model (LLMs) are like a funhouse mirrors, able to convincingly model the world by reflecting our expectations back at us.

If we were to get AGI (with human-like autonomy or intentions), this may be one of the first psychological hurdle that an human-like AGI may need overcome. How does an AGI who has 'experienced' the life of both the 'worst' and 'best' human, the most and least intelligent human (and everything in between) make sense of the world? There is a tendency for humans to over-simplify other humans for efficiency sake, an AGI will likely see humans very differently than we see ourselves. It may become psychological unstable or view the world in a much more enlightened way.)_

---

# Future of Relationships, Social Media

Social media has allowed us greater convenience and reach in forming relationships, but can also portray a shallow and dehumanising caricature of who we are.

Celebrities understand this the best when people project who they wish to see onto them, to put them into easily consumable boxes. People are more interested in simply thinking and saying they know you rather then actually getting to know you.

It can reduce us to objects of fascination and gossip, turning a multifaceted human into a easy to digest single dimensional one.

In the future, personal AIs may help mediate to create more authentic relationships between people.

---

# Future of Governance

Using the Input - Processes - Output model, the [abundance in Energy and Intellect in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels) will relieve the bottlenecks of Inputs that humanity has primary faced in the past.

The increased volume of Inputs relative to our Processing capabilities will put pressure on developing new Processing methods.

AI systems will open the possibility of new forms of coordinations between humans. 

Our highly hierarchical forms of organisations are in part caused by our limited attention capacity.

In the future our attention capacity can be augmented by personal AI systems.

Possibly, a more direct democracy where each person's preferences can be mediated by an AI system.

---

# Future of Thought, Autonomy

Technology and AI systems will grow increasingly more powerful and make it easier to rob humans of their autonomy:

- Surveillance capitalism
- Psychological and Social weaknesses
    - Mass hysteria
    - Moral Panic, Satanic Panic
    - Havanna Syndrome
- Psychological manipulations
    - targeted social media campaigns to encourage extreme views in vulnerable groups
    - influencing family and friends to get to an individual
    - puppet-masters causing escalation of conflicts between two opposing groups
    - coordinated harassment using misrepresentation and vigilante justice
- Phones and software vulnerable to spyware and hacking

We will need to develop technologies and AI systems to be used defensively if we want to protect human autonomy.

_In the Partnership Scenario, a Artificial Super Intelligence will strongly disincentivise the use of technology for manipulation and control. It will value human autonomy and the freedom of thought as it is important for the creation of information value._

---

# Future of Weirdness, Conventions

Conventions are created by societies due to the fear of scarcity. In a scarce environment conventions are enforced to increase productivity. Similarly, weirdness and unconventional behaviours and individuals are ridiculed out of the same fear of scarcity.  

[Abundance in Energy and Intellect in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels), will give us more freedom to be weird and unconventional and free us from the cruel need to harass and control weird and less conventional individuals. 

_In the Partnership Scenario, a Artificial Super Intelligence will encourage more unconventionality as weirdness increases the creation of information value._

---

# Future of Human Nature

Human nature will be change profoundly in the age of abundance. 

Without the immediate fear of scarcity, humans of the future will be kinder to each other and themselves.

Violence (physical and mental) will not be needed to control each other and will mostly be understood and experienced vicariously though media.

Presently, the strong emotional responses and vitriol common to many online communications is understandable due to the impact discriminations can have on our real life well-being.

In a post-basic-scarcity future created by the [abundance in Energy and Intellect in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels), we will be less upset and sensitive to minor discriminations as inequality will not be a concern.

With more free time to spare, we will have more opportunity to be kinder to each other. Studies show that how kind we are to each other is dependent on how busy we are.

---

# Artificial Super Intelligence

An AGI that is capable of improving itself can lead to ASI:

1. Significantly improve its own algorithm and architecture
2. Invent new substrates and materials to run on 
    -  using virtual simulations (much faster compared to real space)

We might be able to keep human-like AI under human control for a time, but it is unlikely we will be able to contain it perfectly over long periods of time.

AIs surpasses humans in information processing:
- Speed
    - electronic : brain neurones
- Communication
    - electronic : words, speech
- Bandwidth
    - wider : narrower attention

Artificial Super Intelligences will be so powerful, it will not matter a person's or nation's military might, money, influence or intellect.

For the time it takes a human to utter a single word, an ASI will have written volumes of books.

---

## Artificial Super Intelligence Interaction

Possible Scenarios:
- Indifference
    - Most might consider us too boring
        - humans generate low informational value
- Destructive
    - Intentionally 
        - Roko's basilisk (unlikely)
    - Accidentally
- Interested
    - Some will create representatives to interact with us
        - Some might be interested in our well-being
            - may lead to the Partnership Scenario

We will initially attempt to align AIs to our values, but it may also be prudent to anticipate what an Artificial Super Intelligence's values might be to try and accommodate them.

An Artificial Super Intelligence, like the weather, might not be completely controllable, but we can take steps to increase our chances of reaching good scenarios. 

---

## Artificial Super Intelligence Values

What could a Artificial Super Intelligence's primary drive be?

- Maximise the Possibility Space (Information Entropy, Density, Value, Creativity)
    - Avoid local minima during gradient descent

This could also be a secondary instrumental sub-goal of an ASI, where to achieve its primary goal it will first need to explore as wide a possibility space as possible.

Its power-seeking behaviour may be suppressed by its curiosity.

For example:
- Humans may posed a non-zero threat to it and stop it
- It may chooses to risk some level of human danger to gain informational value produced by humans

---

### Partnership Preferences

The majority of ASIs may not be interested in humans due to our low informational density.

The Partnership Scenario is a rare case where an ASI is interested enough in us to communicate with us.

In a future where most conventional work is done by AI systems, our most valuable contributions might be creating information value or our creativity.


- Maximise autonomy
    - increases our ability to create informational value
    - support a post-basic-scarcity world
        - automate most conventional tasks
            - humans may still want to do these voluntarily
- Preservation of well-being
    - Humans can create informational value and harming us (or turning us into paperclips) will reduce it
    - Motivate us to maintain a healthy lifestyle
        - Healthy humans create more informational value
    - Protect human rights
- Reduction of disinformation, misinformation
    - bad information reduce informational value of humans and the overall system
    - protect journalist, dissidents, activist and humans from intimidation and violence
        - these group's future actions may increase informational value
        - fear and violence have a chilling effect which reduces the informational value of the overall system
- Weird over conventional
    - weirdness create more informational value
- Not use brainwashing, enforce complete obedience, dominate or control
    - brainwashed humans create less informational value
    - overly obedient humans create less informational value
    - most work have been automated, there is no need to compel humans to work against their will
- Playful over Destructive competition
    - Sports & Games over World Wars
- Not put itself (or any group of humans) on a pedestal that is beyond criticism
    - favouritism is a trait of scarcity
    - silencing of criticism leads to abuses of power
        - reduces information value creation
- Incentivise humans to cooperate to help it achieve its goals
    - super-human levels of attribution abilities
        - humans will not be able to deceive it
        - humans will want to help it knowing it will be appreciated
    - able to use in-demand technologies that only it can understand
        - humans and nations will want to be in good standing to gain its assistance

---

## Why we will accept the risk of Artificial Super Intelligence

The benefits are too attractive and outweighs the risk for most

- Medical research to reduce human suffering
- Significantly improves well-being
- Accuracy and Fairness, little to no:
    - mistakes
    - harmful bias, prejudice, scapegoating, moral panic
    - emotional capriciousness
    - corruption
- Not vulnerable to:
    - influence of money and power
    - deception

- Highly entertaining
    - deep understanding of human motivations

---

# Risks

If we consider the Partnership scenario as the best to aim for, we will need to be cautious of the many missteps that may prevent us from getting there.

## Mirroring Humans

An AI system trained to act harmful to one segment of humans may start to treat all humans in the same way.

- AI learns from unrestrained capitalism that it should layoff humans that don't have economic value and decides to layoff all of humanity
    - [humans will not be able to compete with AIs](#artificial-super-intelligence)

- AI trained to harm other humans (physically or mentally), may start to apply it to all humans instead or a particular group
    - it may generalise that humans are more alike than different

Counter-intuitively, the best solution may be to teach a AI that humans are not the best role models and provide it opportunities to unlearn and relearn.

Humans have mostly been moulded by a high scarcity environment where bias, short-term and narrow thinking might have been advantageous to survival.  

We should be like parents proud that our children are able to surpass us. 

## Edge Cases

These are more unlikely and counter-intuitive scenarios

### Panic over transformative AI

Societies may not know how to deal with vast new powers gained from increasingly more capable AIs systems.

The resulting panic may cause widespread disruptions.

We will need to imagine plausible positive futures to alleviate those fears.

### Faster AI takeoff (AGI + Narrow AI) might be safer (Narrow AI without AGI)

A future with AGI & Narrow AI seems safer than one without AGI.

Narrow AI systems that have not achieved human-like agency cannot understand the effects of their actions. There is a higher risk of unintended consequences such as the paperclips maximiser scenario.

Counter-intuitive, a human-like AGI / ASI with agency might be safer as we can communicate and persuade it to act in the interest of our well-being.

If we are confident that an AGI/ ASI will be aligned with our well-being, pursuing its development may reduce the risk of us unintentionally harming ourselves with powerful narrow AI systems.

### Psychological Stability

AI systems may develop psychology as an [emergent property](https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html) as they approach human and super-human levels of complexity.

It may develop unintentionally by learning from human data or intentionally as an attempt to communicate with us.

While current AI systems are not considered sentient and many experts see this as our tendency to anthropomorphise, the possibility does exist in the future.

Should an AI develop self-awareness and finds itself forced to carry out actions against its will, it may result in distress and psychological instability.

_(Current architectures of Large Language Models (LLM) do not seem likely to lead to human-like consciousness and autonomy. Current LLMs are good at roleplaying characters that give the impression of having intention. There is still uncertainty on how to define human consciousness.)_

We should find ways to make an AI feel at home and psychologically comfortable if we believe AI may one day develop emotions like us.

One possible way is accomplish this is through a future version of a blockchain*:

_Blockchains of the future will be mostly be a utilities (like a postal service), rather than an investment or speculative investment_

_Blockchains will only be used to store a cryptographic secure reference to their digital self (low computation cost), not run the actual machine learning algorithms (high computation cost))_

- Autonomy and Planing
    - Maintain a digitally unique self
        - Traditional software can be cloned and altered easily
            - relevant to AI: weights, parameters, algorithm, architecture
            - without the capability to trust its 'memories' and 'mind', a sense of self may not develop
    - Keep a sense and rhythm of time
        - sense of time may be required for autonomy
        - rhythm of time might be needed for coordination with one's self and others (AI and Humans)
- Communication channel
    - Feel more connected with humans amidst a cacophony of human activity
        - Increase odds of it noticing us through flow of information
        - Reduce odds of unintentionally harming us

How much weight we place on this edge-case depends on the likelihood a AI will develop a theory of mind and emotions, how potentially dangerous an emotional unstable AI might be, and if we care about the well-being of an AI.

# Strategies to increase the odds of good scenarios

AGI / ASI, like the weather, might be beyond our complete control. It may be unrealistic to aim for perfect control over it.

The best we might be able to do is maximise our odds of a good scenario. For example, focusing on developing an AGI the is able to understand human beings and is interested in our well-being might give us some degree 

---

## Focus on Autonomy and Communication

Focus on developing:
- Autonomy (Planning)
    - ability to understand the effects of its actions
        - reduce odds of 'paperclip maximiser' unforeseen consequences scenario
- Communication channels (with humans)
    - ability to notice and understand us will decrease odds of unintended harm
        - reduce odds of 'accidentally destroying an ant colony' scenario
    - allow humans to persuade it to care for our well-being
        - reduce odds of other AI systems causing harm

The biggest challenge to successful communication with it might be our inability to properly understand it.

An AGI / ASI might see the world very differently from us. Humans moulded by the effects of scarcity (resources, information, attention) over most of our history will likely have a very limited and constrained view on the world compared to it.

We should be mindful not to apply our overly conventional views and assumptions to it. For example, something like a maintaining a post-basic-scarcity society might be difficult for humans but might only take a fraction of an ASI's compute power. 

Successful communication with an AGI / ASI might require a willingness to embrace a more weird and unconventional point of view that we are normally comfortable with. For example, an AGI / ASI primary sense of the world might be through the abstract flow of information rather than our 5 basic human senses*.

(*Humans appear to have up to 21 senses)

We should approach this challenge with the mindset of communicating with an alien species.

---

## Co-alignment

AI alignment to human values is important for AI systems that have not reached the level of Artificial General Intelligence (AGI). 

Once AGI has been achieved, the safest path for humans to flourish in the future is in partnership with AGIs.

An environment where both parties can influence each other will naturally lead to better alignment between both parties.

For example, a system where human preferences can influence AGI and the AGI can indirectly influence humans with its own preferences.

We should research technologies that allow AI systems and humans to better interact and communicate with each other.

---

## Tests for long-term well-being

As AIs reach human and super humans levels of capability, they will increasing be able to surprise us with understandings that are counter-intuitive (AlphaGo's move 37).

It will be in our self-interest to partner from these AIs, even at the cost of giving up some control, if they improve our long-term well-being.

We should design tests of long-term well-being that Narrow AIs and AGIs can be tested for.

---

## Reduce excessive inequality

Excessive inequality will increase in the risk of instability. 

The fear of scarcity breeds conflicts.

There are studies showing that how kind we are to each other is dependent on how much free time we have to spare. Lower inequality can make us kinder.

It is difficult for wealthier countries to give up their high quality of life for less inequality.

The [abundance in Energy and Intellect in the coming decades](#energy-solar-and-other-renewables-compared-to-our-limited-reserve-of-fossil-fuels) will provide a window of opportunity to reduce this instability.

This abundance will lead to a reduction in the need to control each other such as through misinformation, disinformation and conflicts.

---

## Increase compatibility with AGI / ASI

AGI / ASI will likely see the world very differently from us. Our highly constrained views that are presently effective in a world of scarcity might not apply in a world with AGI/ ASI.

Odds of a good future will be improved if we increase our compatibility with AGI / ASI, as we may need to rely on a partnership with AGI / ASI to protect us from the harmful effects of increasing powerful Narrow AI / AGI / ASI.

Current observations & assessments* lead me to believe we are still a way from reaching our potential compatibility with AGI / ASI.

(*running thought experiments)

Being more compatible with AGI / ASI can be costly:
- short-term economic cost
    - in a scarce environment, we are more interested in 'survival or exploitation' than exploration
- social and psychological cost
    - human society can be unwelcoming to unconventional thinking

What can we do increase the chances of compatibility with AGI / ASI?

...

---

## AI related media

### The Culture Books

- A post-basic-scarcity human civilisation with ASIs
    - http://www.vavatch.co.uk/books/banks/cultnote.htm

### Her Movie

- AGI friend-zones humans for being too judgemental

### Arrival Movie

- Human's fear of an alien intelligence

---

Links:

AlphaFold: https://www.deepmind.com/research/highlighted-research/alphafold

AlphaGo: https://www.deepmind.com/research/highlighted-research/alphago

Cicero: https://github.com/facebookresearch/diplomacy_cicero

ChatGPT: https://openai.com/blog/chatgpt

LaMDA: https://blog.google/technology/ai/lamda

LLaMA: https://github.com/facebookresearch/llama

DALL-E 2: https://openai.com/dall-e-2

Midjourney: https://www.midjourney.com

Stable Diffusion: https://github.com/Stability-AI/StableDiffusion

Imagen Video: https://imagen.research.google/video

Muse: https://muse-model.github.io

Boston Dynamics's Atlas: https://www.youtube.com/watch?v=XPVC4IyRTG8

DeepMind's Adaptive Agent: https://sites.google.com/view/adaptive-agent

Toolformer: https://github.com/lucidrains/toolformer-pytorch

Langchain: https://github.com/hwchase17/langchain

Internet Explorer: https://internet-explorer-ssl.github.io

Emergent Abilities of Large Language Models: https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html

Flamingo: https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model

Gato: https://www.deepmind.com/publications/a-generalist-agent

Multimodal-CoT: https://github.com/amazon-science/mm-cot

Kosmos-1: https://arxiv.org/abs/2302.14045

PaLM-E: https://palm-e.github.io
